# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jvSLbjeauPbDlW5Fb6h6XaAgGvfTNNE6

# **Tropos (API Feedback Generation)**
#### *This notebook handles set up to Github/Google Drive, model feedback generation, and generates docx files with feedback.*

## **--Set up**
"""

# Commented out IPython magic to ensure Python compatibility.
#######################################################################
# Sets up Google Drive, Github, Github branch
# Installs requirements.txt and needed libraries
#######################################################################
# Mount Google Drive (optional, you'll get a prompt to authorize account)
# from google.colab import drive
# drive.mount('/content/drive')

# Start in root Colab directory to avoid nesting
# %cd /content

# Clone your GitHub repo (replace with your actual repo URL)
!git clone https://github.com/ML-name/project.git
# %cd project

# List all branches (optional, for checking)
# !git branch -a

# Checkout YOUR branch (!!replace "your-branch-name"!!)
!git checkout -b fix/RateLimitError origin/fix/RateLimitError

# %pip install -r requirements.txt --quiet
!pip install --upgrade openai --quiet #for chatgpt
!pip install -q google-generativeai --quiet #for gemini
!pip install anthropic httpx --quiet #for claude

########################################################################
# Default imports and connecting all API keys
#######################################################################
# Add src folder to python path to edit python files
import sys
sys.path.append('/content/project/')

from google.colab import userdata
import os
import anthropic
openai_key = userdata.get("conner").strip()
deepseek_key = userdata.get("deepseek").strip()
llama_key = userdata.get("groq").strip()
claude_key = userdata.get("claude").strip()


os.environ["OPENAI_API_KEY"] = openai_key

from openai import OpenAI
client = OpenAI(api_key=deepseek_key, base_url="https://api.deepseek.com")

os.environ["LLAMA_API_URL"] = "https://api.groq.com/openai/v1"  # For Ollama
os.environ["LLAMA_API_KEY"] = llama_key  #
os.environ["CLAUDE_API_KEY"] = claude_key

import google.generativeai as genai
genai.configure(api_key=userdata.get("gemini").strip())

"""## **--Feedback Generation Prep**"""

#######################################################################
# Ensure past generated output is cleared
# (comment out if do not want)
#######################################################################
from utils.file_utils import clear_directory_if_exists
output_dir = "/content/project/data/generated_output"
clear_directory_if_exists(output_dir)

#######################################################################
# Set up paths for feedback engine
#######################################################################
from tropos import test_feedback_console
reqs = "./data/raw/Requirements.docx"
examples = "./data/raw/Student_Submissions"
targets = "./data/unmarked_raw"
outputs = "./data/generated_output"

"""## **--Currently Running Models**"""

# GPT-4.1
test_feedback_console(
    prompt_type="FewShot",
    model="gpt-4.1",
    requirements_path=reqs,
    example_dir=examples,
    target_dir=targets,
    output_dir=outputs,
    output_mode="raw",
    max_examples=3
)

"""## **-- Different Models** (Commented out for now)"""

# # GPT-4o
# test_feedback_console(
#     prompt_type="FewShot",
#     model="gpt-4o",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )

# # GPT-4.1
# test_feedback_console(
#     prompt_type="FewShot",
#     model="gpt-4.1",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )

# # DeepSeek
# test_feedback_console(
#     prompt_type="FewShot",
#     model="deepseek-chat",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )

# # Gemini
# test_feedback_console(
#     prompt_type="FewShot",
#     model="gemini-1.5-pro-latest",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )

# # Llama (different prompt_type)
# test_feedback_console(
#     prompt_type="FewShot-Llama",
#     model="meta-llama/llama-4-scout-17b-16e-instruct",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )

# # Claude
# test_feedback_console(
#     prompt_type="FewShot",
#     model="claude-3-opus-20240229",
#     requirements_path=reqs,
#     example_dir=examples,
#     target_dir=targets,
#     output_dir=outputs,
#     output_mode="pretty",
#     max_examples=3
# )
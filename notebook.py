# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at

    https://colab.research.google.com/drive/1sjWSls4fiXNXXR7b-szrl7QxTU2dEuvj


# Main Feedback Generation Notebook
This notebook handles preprocessing, model interaction, and feedback generation.

**--Set up: Github, Paths, Imports**
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive (optional, you'll get a prompt to authorize account)
# from google.colab import drive
# drive.mount('/content/drive')

# Start in root Colab directory to avoid nesting
# %cd /content

# Clone your GitHub repo (replace with your actual repo URL)
!git clone https://github.com/ML-name/project.git
# %cd project

# List all branches (optional, for checking)
# !git branch -a

# Checkout YOUR branch (!!replace "your-branch-name"!!)

!git checkout -b updating-gradio origin/updating-gradio


# %pip install -r requirements.txt --quiet
!pip install --upgrade openai --quiet #for chatgpt
!pip install -q google-generativeai --quiet #for gemini
!pip install anthropic httpx --quiet #for claude

# Add src folder to python path to edit python files
import sys
sys.path.append('/content/project/')

from google.colab import userdata
import os
import anthropic
openai_key = userdata.get("conner").strip()
deepseek_key = userdata.get("deepseek").strip()
llama_key = userdata.get("groq").strip()
claude_key = userdata.get("claude").strip()


os.environ["OPENAI_API_KEY"] = openai_key

from openai import OpenAI
client = OpenAI(api_key=deepseek_key, base_url="https://api.deepseek.com")

os.environ["LLAMA_API_URL"] = "https://api.groq.com/openai/v1"  # For Ollama
os.environ["LLAMA_API_KEY"] = llama_key  #
os.environ["CLAUDE_API_KEY"] = claude_key

import google.generativeai as genai
genai.configure(api_key=userdata.get("gemini").strip())

"""get rid of all data or past output files so the new ones can be made"""

# to erase the past data/generated_output files so the new ones can be here
import os
import shutil

output_dir = "/content/project/data/generated_output"

# Delete all files (not folders) in the directory
for filename in os.listdir(output_dir):
    file_path = os.path.join(output_dir, filename)
    if os.path.isfile(file_path):
        os.remove(file_path)

print("âœ… All files deleted from generated_output.")

from tropos import test_feedback_console
#chatGPT
test_feedback_console(
    prompt_type="FewShot",
    model="gpt-4o",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #comment
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

from tropos import test_feedback_console
#chatGPT 4.1
test_feedback_console(
    prompt_type="FewShot",
    model="gpt-4.1-2025-04-14",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #move to 'False' if you dont want all the outpu
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

from tropos import test_feedback_console
#deepseek
test_feedback_console(
    prompt_type="FewShot",
    model="deepseek-chat",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #move to 'False' if you dont want all the output to be visble
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

from tropos import test_feedback_console
#gemini
test_feedback_console(
    prompt_type="FewShot",
    model="gemini-1.5-pro-latest",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #move to 'False' if you dont want all the output to be visble
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

from tropos import test_feedback_console
#LLaMA using groq, free version so smaller rate limits
test_feedback_console(
    prompt_type="FewShot-Llama", #made a different prompt to get llama feedback correctly put into docx
    model="meta-llama/llama-4-scout-17b-16e-instruct",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #makes it so the output is printed to console too for testing purposes
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

from tropos import test_feedback_console
# Claude
test_feedback_console(
    prompt_type="FewShot",
    model="claude-3-opus-20240229",
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #makes it so the output is printed to console too for testing purposes
    max_examples=3 # Set the number of examples taken by the thingyy at once
)

"""**--Install required libraries**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -r requirements.txt --quiet
# %pip install python-docx --quiet

"""**--Import modules (youre working on)**
<br>*each of our classes will be what will merge to this notebook (im p sure)*
<br>only loads what you explicitly request
<br>(this helps keep memory low and import fast)
<br> *the following is an example with my Rubric module*
"""

!python3 ./tropos/app/blocks_ui_starter.py

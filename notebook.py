# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fcarzE0t4vIjogA4XzQJ37_1YrjgAQ9i

# Main Feedback Generation Notebook
This notebook handles preprocessing, model interaction, and feedback generation.

**--Set up: Github, Paths, Imports, Installs**
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive (optional, you'll get a prompt to authorize account)
# from google.colab import drive
# drive.mount('/content/drive')

# Start in root Colab directory to avoid nesting
# %cd /content

# Clone your GitHub repo (replace with your actual repo URL)
!git clone https://github.com/ML-name/project.git
# %cd project

# List all branches (optional, for checking)
# !git branch -a

# Checkout YOUR branch (!!replace "your-branch-name"!!)
!git checkout -b feature/rubric_printout origin/feature/rubric_printout

# %pip install -r requirements.txt --quiet
# %pip install --upgrade openai --quiet

"""**--Default imports**"""

# Add src folder to python path to edit python files
import sys
sys.path.append('/content/project/')

from google.colab import userdata
import os
openai_key = userdata.get("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = openai_key

"""**--Import modules (youâ€™re working on)**
<br>*each of our classes will be what will merge to this notebook (I'm pretty sure)*
<br>only loads what you explicitly request
<br>(this helps keep memory low and import fast)
<br> *the following is an example with my Rubric module*

**--Lauren's bit that can run the feedback generator here to test**
- ended up modularizing code to prepare for gradio/UI use.
  - had all files include ability to use uploaded files, used local testing files as default to be able to test
  - created feedback_engine file to be able to create runnable program in notebook to test
  - TODO: create runnable feedback program for Gradio/UI when that time comes
"""

from utils.debug_helpers import explore_examples
 # to see what is all being assigned to prompt when cycled.

 explore_examples(
     requirements_path="/content/project/data/raw/Requirements.docx",
     example_dir="/content/project/data/raw/Student_Submissions"
 )

from tropos import run_feedback_batch

#change parameters here for prompt/model/max_examples
run_feedback_batch(
    prompt_type="FewShot",
    model="gpt-4o", #change models here
    requirements_path="./data/raw/Requirements.docx",
    example_dir="./data/raw/Student_Submissions",
    target_dir="./data/unmarked_raw",
    output_dir="./data/generated_output",
    verbose=True, #move to 'False' if you dont want all the output to be visble
    max_examples=4 #change this for RateLimitErrors ('None' to allow for all)
)
# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19zWlie98Z-eS52ZvEBSH6JuWLjmpNQOb

# Main Feedback Generation Notebook
This notebook handles preprocessing, model interaction, and feedback generation.

**--Set up: Github, Paths, Imports, Installs**
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount Google Drive (optional, you'll get a prompt to authorize account)
# from google.colab import drive
# drive.mount('/content/drive')

# Start in root Colab directory to avoid nesting
# %cd /content

# Clone your GitHub repo (replace with your actual repo URL)
!git clone https://github.com/ML-name/project.git
# %cd project

# List all branches (optional, for checking)
# !git branch -a

# Checkout YOUR branch (!!replace "your-branch-name"!!)
!git checkout -b feature/generated_rubric origin/feature/generated_rubric

# %pip install -r requirements.txt --quiet
# %pip install --upgrade openai --quiet

"""**--Default imports**"""

# Add src folder to python path to edit python files
import sys
sys.path.append('/content/project/')

from google.colab import userdata
import os
openai_key = userdata.get("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = openai_key

"""**--Import modules (you‚Äôre working on)**
<br>*each of our classes will be what will merge to this notebook (I'm pretty sure)*
<br>only loads what you explicitly request
<br>(this helps keep memory low and import fast)
<br> *the following is an example with my Rubric module*

**--Lauren's bit that can run the feedback generator here to test**
- ended up modularizing code to prepare for gradio/UI use.
  - had all files include ability to use uploaded files, used local testing files as default to be able to test
  - created feedback_engine file to be able to create runnable program in notebook to test
  - TODO: create runnable feedback program for Gradio/UI when that time comes
"""

# from tropos.preprocess_docx import StudentSubmission
# from utils.feedback_formatting import format_feedback_blocks
#
# # Provide paths to your example file and requirements file
# example_path = "/content/project/data/raw/Student_1/Student_1_Part_1.docx"
# requirements_path = "/content/project/data/raw/Requirements.docx"
#
# # Load the student submission
# example_submission = StudentSubmission(example_path, requirements_path)
#
# # Print all key parts for visual verification
# print("üìÑ Submission Text:\n", example_submission.get_submission_text()[:1000])
# print("\nüóíÔ∏è Comments Text:\n", example_submission.get_comments_text())
# print("\nüìå Assignment Requirements:\n", example_submission.get_requirements_text()[:1000])
# print("\nüìã Rubric:\n", example_submission.get_rubric_prompt())
#

from tropos.models.prompt_builder import build_prompt

# Let's say you're testing FewShot (you can switch to ZeroShot or OneShot too)
prompt_type = "FewShot"

# # Simulate multiple examples (can be just one for testing)
# example_submissions = [example_submission]  # reuse same one for now
#
# # Build the actual prompt
# prompt = build_prompt(prompt_type, examples=example_submissions, target=example_submission)
#
# # Print the result to verify
# print("\nüß™ FINAL PROMPT OUTPUT:\n")
# print(prompt)  # trim if it's huge

from tropos import test_feedback_console

# test_feedback_console(
#     requirements_path="/content/project/data/raw/Requirements.docx",
#     example_dir="/content/project/data/raw",
#     target_dir="/content/project/data/unmarked_raw",
#     output_dir="/content/project/data/generated_output",
#     verbose=True
# )

# from utils.student_loader import load_all_student_examples_recursive
#
# example_dir = "/content/project/data/raw"
# requirements_path = "/content/project/data/raw/Requirements.docx"
#
# examples = load_all_student_examples_recursive(example_dir, requirements_path, verbose=True)
# print(len(examples), "examples loaded.")

# import zipfile
#
# with zipfile.ZipFile("/content/project/data/raw/Student_1/Student_1_Part_1.docx") as z:
#     print(z.namelist())
#

def explore_examples(requirements_path, example_dir):
    from utils.student_loader import load_all_student_examples_recursive
    from tropos.preprocess_docx.assignment_requirements import parse_requirements
    from docx import Document

    print("\nüìå Assignment Requirements:\n")
    parsed_req = parse_requirements(requirements_path)
    print(parsed_req.get_instructions())

    examples = load_all_student_examples_recursive(example_dir, requirements_path)

    print(f"\nüìã Rubric:\n{examples[0].rubric.format_clean_only()}\n")

    for i, example in enumerate(examples):
        print(f"\nüîç Example {i+1}: {example.submission_path}")

        print("\nüìù Submission Text:\n")
        print(example.get_submission_text()[:1000])  # prevent flooding output

        print("\nüí¨ Inline Feedback (Instructor Comments):\n")
        print(example.get_comments_text() or "[No comments found]")

        print("\nüìã Rubric Feedback:\n")
        print(example.get_rubric_feedback_prompt() or "[No rubric feedback found]")

        print("\n" + "-"*80)

#from tropos import explore_examples

explore_examples(
    requirements_path="/content/project/data/Requirements.docx",
    example_dir="/content/project/data/raw"
)
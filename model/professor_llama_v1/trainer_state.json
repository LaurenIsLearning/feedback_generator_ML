{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 280,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 9.39728832244873,
      "learning_rate": 0.0,
      "loss": 4.101,
      "step": 1
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 9.226171493530273,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 4.0077,
      "step": 2
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.172581672668457,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 3.9808,
      "step": 3
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 9.600614547729492,
      "learning_rate": 8.999999999999999e-05,
      "loss": 3.8774,
      "step": 4
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 6.4116291999816895,
      "learning_rate": 0.00011999999999999999,
      "loss": 3.7536,
      "step": 5
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 10.770184516906738,
      "learning_rate": 0.00015,
      "loss": 3.5947,
      "step": 6
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.801724433898926,
      "learning_rate": 0.00017999999999999998,
      "loss": 3.4317,
      "step": 7
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 2.547253370285034,
      "learning_rate": 0.00020999999999999998,
      "loss": 3.1712,
      "step": 8
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 2.2949678897857666,
      "learning_rate": 0.00023999999999999998,
      "loss": 3.0194,
      "step": 9
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 3.5268232822418213,
      "learning_rate": 0.00027,
      "loss": 3.233,
      "step": 10
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.9501736164093018,
      "learning_rate": 0.0003,
      "loss": 2.8548,
      "step": 11
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 2.0799477100372314,
      "learning_rate": 0.0002988888888888889,
      "loss": 2.8828,
      "step": 12
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.8358256816864014,
      "learning_rate": 0.0002977777777777777,
      "loss": 2.7933,
      "step": 13
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.9842665195465088,
      "learning_rate": 0.00029666666666666665,
      "loss": 2.5367,
      "step": 14
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.6533644199371338,
      "learning_rate": 0.0002955555555555555,
      "loss": 2.3444,
      "step": 15
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 1.7726913690567017,
      "learning_rate": 0.00029444444444444445,
      "loss": 2.2225,
      "step": 16
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 1.9558546543121338,
      "learning_rate": 0.00029333333333333327,
      "loss": 2.4844,
      "step": 17
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.994088888168335,
      "learning_rate": 0.0002922222222222222,
      "loss": 2.3443,
      "step": 18
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 2.0402612686157227,
      "learning_rate": 0.0002911111111111111,
      "loss": 2.0462,
      "step": 19
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 2.3056771755218506,
      "learning_rate": 0.00029,
      "loss": 2.5351,
      "step": 20
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9282538890838623,
      "learning_rate": 0.0002888888888888888,
      "loss": 2.2788,
      "step": 21
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 1.7762140035629272,
      "learning_rate": 0.00028777777777777775,
      "loss": 1.8091,
      "step": 22
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 2.0046584606170654,
      "learning_rate": 0.0002866666666666667,
      "loss": 1.9353,
      "step": 23
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.8706645965576172,
      "learning_rate": 0.00028555555555555555,
      "loss": 1.2703,
      "step": 24
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 2.0498905181884766,
      "learning_rate": 0.0002844444444444444,
      "loss": 1.7591,
      "step": 25
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 2.6282737255096436,
      "learning_rate": 0.0002833333333333333,
      "loss": 1.8693,
      "step": 26
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 2.4383697509765625,
      "learning_rate": 0.00028222222222222223,
      "loss": 1.6658,
      "step": 27
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.364912509918213,
      "learning_rate": 0.0002811111111111111,
      "loss": 1.8076,
      "step": 28
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 2.4600918292999268,
      "learning_rate": 0.00028,
      "loss": 1.5676,
      "step": 29
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 2.974727153778076,
      "learning_rate": 0.00027888888888888885,
      "loss": 1.3483,
      "step": 30
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 2.550105094909668,
      "learning_rate": 0.0002777777777777778,
      "loss": 1.344,
      "step": 31
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 2.5705554485321045,
      "learning_rate": 0.00027666666666666665,
      "loss": 1.1,
      "step": 32
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 2.1189358234405518,
      "learning_rate": 0.0002755555555555555,
      "loss": 0.9566,
      "step": 33
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 2.6278839111328125,
      "learning_rate": 0.00027444444444444445,
      "loss": 1.3202,
      "step": 34
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.9219987392425537,
      "learning_rate": 0.00027333333333333333,
      "loss": 1.1726,
      "step": 35
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 2.3597586154937744,
      "learning_rate": 0.0002722222222222222,
      "loss": 0.9167,
      "step": 36
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 2.3945798873901367,
      "learning_rate": 0.0002711111111111111,
      "loss": 0.69,
      "step": 37
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 2.7577474117279053,
      "learning_rate": 0.00027,
      "loss": 0.8593,
      "step": 38
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 2.698718786239624,
      "learning_rate": 0.0002688888888888889,
      "loss": 0.7564,
      "step": 39
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 3.1731042861938477,
      "learning_rate": 0.00026777777777777775,
      "loss": 0.9557,
      "step": 40
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 2.8551528453826904,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.9077,
      "step": 41
    },
    {
      "epoch": 6.0,
      "grad_norm": 2.8086369037628174,
      "learning_rate": 0.00026555555555555555,
      "loss": 0.9233,
      "step": 42
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 2.8288910388946533,
      "learning_rate": 0.00026444444444444443,
      "loss": 0.6621,
      "step": 43
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 2.3631908893585205,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.5408,
      "step": 44
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 2.482635259628296,
      "learning_rate": 0.00026222222222222223,
      "loss": 0.7406,
      "step": 45
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 2.5079522132873535,
      "learning_rate": 0.0002611111111111111,
      "loss": 0.4298,
      "step": 46
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 4.670334815979004,
      "learning_rate": 0.00026,
      "loss": 0.65,
      "step": 47
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 6.519153118133545,
      "learning_rate": 0.00025888888888888885,
      "loss": 0.6311,
      "step": 48
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.344679594039917,
      "learning_rate": 0.0002577777777777778,
      "loss": 0.5286,
      "step": 49
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 2.6047399044036865,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.4498,
      "step": 50
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 2.082378387451172,
      "learning_rate": 0.00025555555555555553,
      "loss": 0.3204,
      "step": 51
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 2.2801220417022705,
      "learning_rate": 0.0002544444444444444,
      "loss": 0.3827,
      "step": 52
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 1.9105123281478882,
      "learning_rate": 0.00025333333333333333,
      "loss": 0.2803,
      "step": 53
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 2.1972692012786865,
      "learning_rate": 0.0002522222222222222,
      "loss": 0.3835,
      "step": 54
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 2.4099068641662598,
      "learning_rate": 0.0002511111111111111,
      "loss": 0.3521,
      "step": 55
    },
    {
      "epoch": 8.0,
      "grad_norm": 2.4955358505249023,
      "learning_rate": 0.00025,
      "loss": 0.3638,
      "step": 56
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 1.4915226697921753,
      "learning_rate": 0.0002488888888888889,
      "loss": 0.2047,
      "step": 57
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 1.9071033000946045,
      "learning_rate": 0.00024777777777777775,
      "loss": 0.2357,
      "step": 58
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 2.6341116428375244,
      "learning_rate": 0.0002466666666666666,
      "loss": 0.3078,
      "step": 59
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 1.8831279277801514,
      "learning_rate": 0.00024555555555555556,
      "loss": 0.2007,
      "step": 60
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 2.66089129447937,
      "learning_rate": 0.00024444444444444443,
      "loss": 0.2846,
      "step": 61
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 2.1396703720092773,
      "learning_rate": 0.0002433333333333333,
      "loss": 0.2585,
      "step": 62
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.319115161895752,
      "learning_rate": 0.0002422222222222222,
      "loss": 0.2439,
      "step": 63
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 1.4195979833602905,
      "learning_rate": 0.00024111111111111108,
      "loss": 0.172,
      "step": 64
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 1.8224306106567383,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.1834,
      "step": 65
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 1.69451904296875,
      "learning_rate": 0.00023888888888888885,
      "loss": 0.1641,
      "step": 66
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 2.0969178676605225,
      "learning_rate": 0.00023777777777777775,
      "loss": 0.1761,
      "step": 67
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 1.1844223737716675,
      "learning_rate": 0.00023666666666666663,
      "loss": 0.1063,
      "step": 68
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 2.560634136199951,
      "learning_rate": 0.00023555555555555553,
      "loss": 0.2219,
      "step": 69
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.8801910877227783,
      "learning_rate": 0.0002344444444444444,
      "loss": 0.1372,
      "step": 70
    },
    {
      "epoch": 10.142857142857142,
      "grad_norm": 1.5264517068862915,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.1085,
      "step": 71
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 1.5209788084030151,
      "learning_rate": 0.00023222222222222218,
      "loss": 0.1039,
      "step": 72
    },
    {
      "epoch": 10.428571428571429,
      "grad_norm": 1.4482663869857788,
      "learning_rate": 0.00023111111111111108,
      "loss": 0.1144,
      "step": 73
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 1.99063241481781,
      "learning_rate": 0.00023,
      "loss": 0.1413,
      "step": 74
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 1.5613311529159546,
      "learning_rate": 0.00022888888888888885,
      "loss": 0.1087,
      "step": 75
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 1.8424785137176514,
      "learning_rate": 0.00022777777777777778,
      "loss": 0.1332,
      "step": 76
    },
    {
      "epoch": 11.0,
      "grad_norm": 1.5996218919754028,
      "learning_rate": 0.00022666666666666663,
      "loss": 0.1131,
      "step": 77
    },
    {
      "epoch": 11.142857142857142,
      "grad_norm": 1.3877760171890259,
      "learning_rate": 0.00022555555555555556,
      "loss": 0.1039,
      "step": 78
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 1.0148406028747559,
      "learning_rate": 0.0002244444444444444,
      "loss": 0.0743,
      "step": 79
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 2.4689383506774902,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.1597,
      "step": 80
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 1.0362993478775024,
      "learning_rate": 0.00022222222222222218,
      "loss": 0.0742,
      "step": 81
    },
    {
      "epoch": 11.714285714285714,
      "grad_norm": 1.5211760997772217,
      "learning_rate": 0.0002211111111111111,
      "loss": 0.1117,
      "step": 82
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 1.3651161193847656,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.0989,
      "step": 83
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.468875765800476,
      "learning_rate": 0.00021888888888888888,
      "loss": 0.1118,
      "step": 84
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 2.243645429611206,
      "learning_rate": 0.00021777777777777778,
      "loss": 0.1594,
      "step": 85
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 1.477745532989502,
      "learning_rate": 0.00021666666666666666,
      "loss": 0.1009,
      "step": 86
    },
    {
      "epoch": 12.428571428571429,
      "grad_norm": 1.279667615890503,
      "learning_rate": 0.00021555555555555556,
      "loss": 0.094,
      "step": 87
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 0.9550707340240479,
      "learning_rate": 0.00021444444444444443,
      "loss": 0.0641,
      "step": 88
    },
    {
      "epoch": 12.714285714285714,
      "grad_norm": 1.4904780387878418,
      "learning_rate": 0.00021333333333333333,
      "loss": 0.0935,
      "step": 89
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 1.671081304550171,
      "learning_rate": 0.0002122222222222222,
      "loss": 0.0905,
      "step": 90
    },
    {
      "epoch": 13.0,
      "grad_norm": 1.1017699241638184,
      "learning_rate": 0.0002111111111111111,
      "loss": 0.0642,
      "step": 91
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 1.1104546785354614,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.0634,
      "step": 92
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 1.0817137956619263,
      "learning_rate": 0.00020888888888888888,
      "loss": 0.0579,
      "step": 93
    },
    {
      "epoch": 13.428571428571429,
      "grad_norm": 1.1391218900680542,
      "learning_rate": 0.00020777777777777776,
      "loss": 0.0594,
      "step": 94
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 1.3725404739379883,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.0717,
      "step": 95
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 1.1366113424301147,
      "learning_rate": 0.00020555555555555556,
      "loss": 0.0643,
      "step": 96
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 1.3844386339187622,
      "learning_rate": 0.00020444444444444443,
      "loss": 0.0788,
      "step": 97
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.2654513120651245,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.0688,
      "step": 98
    },
    {
      "epoch": 14.142857142857142,
      "grad_norm": 1.9684438705444336,
      "learning_rate": 0.0002022222222222222,
      "loss": 0.0611,
      "step": 99
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 0.8409067988395691,
      "learning_rate": 0.0002011111111111111,
      "loss": 0.0553,
      "step": 100
    },
    {
      "epoch": 14.428571428571429,
      "grad_norm": 0.8636556267738342,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.0571,
      "step": 101
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 1.0219008922576904,
      "learning_rate": 0.00019888888888888888,
      "loss": 0.0535,
      "step": 102
    },
    {
      "epoch": 14.714285714285714,
      "grad_norm": 0.8063045740127563,
      "learning_rate": 0.00019777777777777776,
      "loss": 0.0502,
      "step": 103
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 1.2136870622634888,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.0615,
      "step": 104
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.346906304359436,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.061,
      "step": 105
    },
    {
      "epoch": 15.142857142857142,
      "grad_norm": 0.7363104820251465,
      "learning_rate": 0.00019444444444444443,
      "loss": 0.0414,
      "step": 106
    },
    {
      "epoch": 15.285714285714286,
      "grad_norm": 1.451712965965271,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.0685,
      "step": 107
    },
    {
      "epoch": 15.428571428571429,
      "grad_norm": 1.0297269821166992,
      "learning_rate": 0.0001922222222222222,
      "loss": 0.0446,
      "step": 108
    },
    {
      "epoch": 15.571428571428571,
      "grad_norm": 0.7112764120101929,
      "learning_rate": 0.0001911111111111111,
      "loss": 0.044,
      "step": 109
    },
    {
      "epoch": 15.714285714285714,
      "grad_norm": 1.478968858718872,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0675,
      "step": 110
    },
    {
      "epoch": 15.857142857142858,
      "grad_norm": 1.3131299018859863,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.0751,
      "step": 111
    },
    {
      "epoch": 16.0,
      "grad_norm": 1.2940303087234497,
      "learning_rate": 0.00018777777777777776,
      "loss": 0.0553,
      "step": 112
    },
    {
      "epoch": 16.142857142857142,
      "grad_norm": 0.8543548583984375,
      "learning_rate": 0.00018666666666666666,
      "loss": 0.047,
      "step": 113
    },
    {
      "epoch": 16.285714285714285,
      "grad_norm": 0.5551177263259888,
      "learning_rate": 0.00018555555555555553,
      "loss": 0.0392,
      "step": 114
    },
    {
      "epoch": 16.428571428571427,
      "grad_norm": 0.9068898558616638,
      "learning_rate": 0.00018444444444444443,
      "loss": 0.0481,
      "step": 115
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 1.1812468767166138,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.0568,
      "step": 116
    },
    {
      "epoch": 16.714285714285715,
      "grad_norm": 1.204451560974121,
      "learning_rate": 0.0001822222222222222,
      "loss": 0.0537,
      "step": 117
    },
    {
      "epoch": 16.857142857142858,
      "grad_norm": 1.650098443031311,
      "learning_rate": 0.0001811111111111111,
      "loss": 0.0757,
      "step": 118
    },
    {
      "epoch": 17.0,
      "grad_norm": 1.2860785722732544,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0545,
      "step": 119
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 0.7223072052001953,
      "learning_rate": 0.00017888888888888889,
      "loss": 0.0447,
      "step": 120
    },
    {
      "epoch": 17.285714285714285,
      "grad_norm": 0.6236304640769958,
      "learning_rate": 0.00017777777777777776,
      "loss": 0.0385,
      "step": 121
    },
    {
      "epoch": 17.428571428571427,
      "grad_norm": 0.6973855495452881,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.0354,
      "step": 122
    },
    {
      "epoch": 17.571428571428573,
      "grad_norm": 0.7451120018959045,
      "learning_rate": 0.00017555555555555553,
      "loss": 0.0492,
      "step": 123
    },
    {
      "epoch": 17.714285714285715,
      "grad_norm": 0.7751985192298889,
      "learning_rate": 0.00017444444444444444,
      "loss": 0.0411,
      "step": 124
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 1.0419517755508423,
      "learning_rate": 0.0001733333333333333,
      "loss": 0.0432,
      "step": 125
    },
    {
      "epoch": 18.0,
      "grad_norm": 1.2437342405319214,
      "learning_rate": 0.0001722222222222222,
      "loss": 0.0528,
      "step": 126
    },
    {
      "epoch": 18.142857142857142,
      "grad_norm": 0.8579031825065613,
      "learning_rate": 0.0001711111111111111,
      "loss": 0.0378,
      "step": 127
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 0.5265470743179321,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0387,
      "step": 128
    },
    {
      "epoch": 18.428571428571427,
      "grad_norm": 0.536386251449585,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.0336,
      "step": 129
    },
    {
      "epoch": 18.571428571428573,
      "grad_norm": 0.8833695650100708,
      "learning_rate": 0.00016777777777777776,
      "loss": 0.0425,
      "step": 130
    },
    {
      "epoch": 18.714285714285715,
      "grad_norm": 0.9279812574386597,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.0404,
      "step": 131
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 0.8051803708076477,
      "learning_rate": 0.00016555555555555554,
      "loss": 0.0463,
      "step": 132
    },
    {
      "epoch": 19.0,
      "grad_norm": 1.2385164499282837,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.0519,
      "step": 133
    },
    {
      "epoch": 19.142857142857142,
      "grad_norm": 0.8697840571403503,
      "learning_rate": 0.0001633333333333333,
      "loss": 0.0415,
      "step": 134
    },
    {
      "epoch": 19.285714285714285,
      "grad_norm": 0.6029960513114929,
      "learning_rate": 0.0001622222222222222,
      "loss": 0.0359,
      "step": 135
    },
    {
      "epoch": 19.428571428571427,
      "grad_norm": 0.6190552711486816,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.0331,
      "step": 136
    },
    {
      "epoch": 19.571428571428573,
      "grad_norm": 0.8851109743118286,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.0432,
      "step": 137
    },
    {
      "epoch": 19.714285714285715,
      "grad_norm": 0.577323853969574,
      "learning_rate": 0.0001588888888888889,
      "loss": 0.0351,
      "step": 138
    },
    {
      "epoch": 19.857142857142858,
      "grad_norm": 0.5120869874954224,
      "learning_rate": 0.00015777777777777776,
      "loss": 0.0342,
      "step": 139
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.8253043293952942,
      "learning_rate": 0.00015666666666666666,
      "loss": 0.0377,
      "step": 140
    },
    {
      "epoch": 20.142857142857142,
      "grad_norm": 0.5098446011543274,
      "learning_rate": 0.00015555555555555554,
      "loss": 0.0272,
      "step": 141
    },
    {
      "epoch": 20.285714285714285,
      "grad_norm": 0.6534463167190552,
      "learning_rate": 0.00015444444444444444,
      "loss": 0.033,
      "step": 142
    },
    {
      "epoch": 20.428571428571427,
      "grad_norm": 0.5124753713607788,
      "learning_rate": 0.0001533333333333333,
      "loss": 0.0343,
      "step": 143
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 1.1760766506195068,
      "learning_rate": 0.0001522222222222222,
      "loss": 0.0471,
      "step": 144
    },
    {
      "epoch": 20.714285714285715,
      "grad_norm": 1.0180816650390625,
      "learning_rate": 0.00015111111111111109,
      "loss": 0.0454,
      "step": 145
    },
    {
      "epoch": 20.857142857142858,
      "grad_norm": 0.6167316436767578,
      "learning_rate": 0.00015,
      "loss": 0.04,
      "step": 146
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.6706995964050293,
      "learning_rate": 0.00014888888888888886,
      "loss": 0.0383,
      "step": 147
    },
    {
      "epoch": 21.142857142857142,
      "grad_norm": 0.3216092586517334,
      "learning_rate": 0.00014777777777777776,
      "loss": 0.0297,
      "step": 148
    },
    {
      "epoch": 21.285714285714285,
      "grad_norm": 0.8638289570808411,
      "learning_rate": 0.00014666666666666664,
      "loss": 0.0398,
      "step": 149
    },
    {
      "epoch": 21.428571428571427,
      "grad_norm": 1.1368143558502197,
      "learning_rate": 0.00014555555555555554,
      "loss": 0.0479,
      "step": 150
    },
    {
      "epoch": 21.571428571428573,
      "grad_norm": 0.5284157395362854,
      "learning_rate": 0.0001444444444444444,
      "loss": 0.0395,
      "step": 151
    },
    {
      "epoch": 21.714285714285715,
      "grad_norm": 0.7021481990814209,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.0355,
      "step": 152
    },
    {
      "epoch": 21.857142857142858,
      "grad_norm": 0.9652605652809143,
      "learning_rate": 0.0001422222222222222,
      "loss": 0.0536,
      "step": 153
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.412676602602005,
      "learning_rate": 0.00014111111111111111,
      "loss": 0.0335,
      "step": 154
    },
    {
      "epoch": 22.142857142857142,
      "grad_norm": 0.7882108688354492,
      "learning_rate": 0.00014,
      "loss": 0.0377,
      "step": 155
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 0.4249023199081421,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.0308,
      "step": 156
    },
    {
      "epoch": 22.428571428571427,
      "grad_norm": 0.6587106585502625,
      "learning_rate": 0.00013777777777777776,
      "loss": 0.0334,
      "step": 157
    },
    {
      "epoch": 22.571428571428573,
      "grad_norm": 0.4122422933578491,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.0265,
      "step": 158
    },
    {
      "epoch": 22.714285714285715,
      "grad_norm": 0.5596497654914856,
      "learning_rate": 0.00013555555555555554,
      "loss": 0.0374,
      "step": 159
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 0.5217379331588745,
      "learning_rate": 0.00013444444444444444,
      "loss": 0.0409,
      "step": 160
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.39657139778137207,
      "learning_rate": 0.0001333333333333333,
      "loss": 0.0312,
      "step": 161
    },
    {
      "epoch": 23.142857142857142,
      "grad_norm": 0.6087044477462769,
      "learning_rate": 0.00013222222222222221,
      "loss": 0.0298,
      "step": 162
    },
    {
      "epoch": 23.285714285714285,
      "grad_norm": 0.5250682234764099,
      "learning_rate": 0.00013111111111111111,
      "loss": 0.0313,
      "step": 163
    },
    {
      "epoch": 23.428571428571427,
      "grad_norm": 0.28700244426727295,
      "learning_rate": 0.00013,
      "loss": 0.0271,
      "step": 164
    },
    {
      "epoch": 23.571428571428573,
      "grad_norm": 0.805519700050354,
      "learning_rate": 0.0001288888888888889,
      "loss": 0.0353,
      "step": 165
    },
    {
      "epoch": 23.714285714285715,
      "grad_norm": 0.4001779854297638,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.0257,
      "step": 166
    },
    {
      "epoch": 23.857142857142858,
      "grad_norm": 0.621780514717102,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.0429,
      "step": 167
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.7389363646507263,
      "learning_rate": 0.00012555555555555554,
      "loss": 0.0391,
      "step": 168
    },
    {
      "epoch": 24.142857142857142,
      "grad_norm": 0.3416045904159546,
      "learning_rate": 0.00012444444444444444,
      "loss": 0.0246,
      "step": 169
    },
    {
      "epoch": 24.285714285714285,
      "grad_norm": 0.21201226115226746,
      "learning_rate": 0.0001233333333333333,
      "loss": 0.0268,
      "step": 170
    },
    {
      "epoch": 24.428571428571427,
      "grad_norm": 0.4721803665161133,
      "learning_rate": 0.00012222222222222221,
      "loss": 0.0333,
      "step": 171
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 0.7802262902259827,
      "learning_rate": 0.0001211111111111111,
      "loss": 0.0335,
      "step": 172
    },
    {
      "epoch": 24.714285714285715,
      "grad_norm": 0.814913272857666,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0444,
      "step": 173
    },
    {
      "epoch": 24.857142857142858,
      "grad_norm": 0.48727989196777344,
      "learning_rate": 0.00011888888888888888,
      "loss": 0.0305,
      "step": 174
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.6459466218948364,
      "learning_rate": 0.00011777777777777776,
      "loss": 0.0293,
      "step": 175
    },
    {
      "epoch": 25.142857142857142,
      "grad_norm": 0.5997887253761292,
      "learning_rate": 0.00011666666666666665,
      "loss": 0.0437,
      "step": 176
    },
    {
      "epoch": 25.285714285714285,
      "grad_norm": 0.3657154142856598,
      "learning_rate": 0.00011555555555555554,
      "loss": 0.0295,
      "step": 177
    },
    {
      "epoch": 25.428571428571427,
      "grad_norm": 0.345686137676239,
      "learning_rate": 0.00011444444444444443,
      "loss": 0.0253,
      "step": 178
    },
    {
      "epoch": 25.571428571428573,
      "grad_norm": 0.2861981987953186,
      "learning_rate": 0.00011333333333333331,
      "loss": 0.0279,
      "step": 179
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 0.7155672907829285,
      "learning_rate": 0.0001122222222222222,
      "loss": 0.0313,
      "step": 180
    },
    {
      "epoch": 25.857142857142858,
      "grad_norm": 0.420549213886261,
      "learning_rate": 0.00011111111111111109,
      "loss": 0.03,
      "step": 181
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.4039318859577179,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.0296,
      "step": 182
    },
    {
      "epoch": 26.142857142857142,
      "grad_norm": 0.20876479148864746,
      "learning_rate": 0.00010888888888888889,
      "loss": 0.0265,
      "step": 183
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 0.20222625136375427,
      "learning_rate": 0.00010777777777777778,
      "loss": 0.0252,
      "step": 184
    },
    {
      "epoch": 26.428571428571427,
      "grad_norm": 0.4046470820903778,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.0244,
      "step": 185
    },
    {
      "epoch": 26.571428571428573,
      "grad_norm": 0.3200215697288513,
      "learning_rate": 0.00010555555555555555,
      "loss": 0.0252,
      "step": 186
    },
    {
      "epoch": 26.714285714285715,
      "grad_norm": 0.27406007051467896,
      "learning_rate": 0.00010444444444444444,
      "loss": 0.0275,
      "step": 187
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 0.30888447165489197,
      "learning_rate": 0.00010333333333333333,
      "loss": 0.0297,
      "step": 188
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.2796824872493744,
      "learning_rate": 0.00010222222222222222,
      "loss": 0.032,
      "step": 189
    },
    {
      "epoch": 27.142857142857142,
      "grad_norm": 0.36054834723472595,
      "learning_rate": 0.0001011111111111111,
      "loss": 0.0323,
      "step": 190
    },
    {
      "epoch": 27.285714285714285,
      "grad_norm": 0.2925373613834381,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.0289,
      "step": 191
    },
    {
      "epoch": 27.428571428571427,
      "grad_norm": 0.3163260817527771,
      "learning_rate": 9.888888888888888e-05,
      "loss": 0.0237,
      "step": 192
    },
    {
      "epoch": 27.571428571428573,
      "grad_norm": 0.26628750562667847,
      "learning_rate": 9.777777777777778e-05,
      "loss": 0.0255,
      "step": 193
    },
    {
      "epoch": 27.714285714285715,
      "grad_norm": 0.23398827016353607,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.0264,
      "step": 194
    },
    {
      "epoch": 27.857142857142858,
      "grad_norm": 0.533342719078064,
      "learning_rate": 9.555555555555555e-05,
      "loss": 0.0347,
      "step": 195
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.6576440930366516,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.0308,
      "step": 196
    },
    {
      "epoch": 28.142857142857142,
      "grad_norm": 0.4880393445491791,
      "learning_rate": 9.333333333333333e-05,
      "loss": 0.0288,
      "step": 197
    },
    {
      "epoch": 28.285714285714285,
      "grad_norm": 0.20905421674251556,
      "learning_rate": 9.222222222222222e-05,
      "loss": 0.0271,
      "step": 198
    },
    {
      "epoch": 28.428571428571427,
      "grad_norm": 0.21148067712783813,
      "learning_rate": 9.11111111111111e-05,
      "loss": 0.0259,
      "step": 199
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 0.33822906017303467,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0288,
      "step": 200
    },
    {
      "epoch": 28.714285714285715,
      "grad_norm": 0.31934940814971924,
      "learning_rate": 8.888888888888888e-05,
      "loss": 0.0243,
      "step": 201
    },
    {
      "epoch": 28.857142857142858,
      "grad_norm": 0.39383333921432495,
      "learning_rate": 8.777777777777777e-05,
      "loss": 0.0241,
      "step": 202
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.22624899446964264,
      "learning_rate": 8.666666666666665e-05,
      "loss": 0.0289,
      "step": 203
    },
    {
      "epoch": 29.142857142857142,
      "grad_norm": 0.6104301810264587,
      "learning_rate": 8.555555555555556e-05,
      "loss": 0.0295,
      "step": 204
    },
    {
      "epoch": 29.285714285714285,
      "grad_norm": 0.29419368505477905,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.0253,
      "step": 205
    },
    {
      "epoch": 29.428571428571427,
      "grad_norm": 0.2651226818561554,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.027,
      "step": 206
    },
    {
      "epoch": 29.571428571428573,
      "grad_norm": 1.0559362173080444,
      "learning_rate": 8.222222222222222e-05,
      "loss": 0.0317,
      "step": 207
    },
    {
      "epoch": 29.714285714285715,
      "grad_norm": 0.22476203739643097,
      "learning_rate": 8.11111111111111e-05,
      "loss": 0.0275,
      "step": 208
    },
    {
      "epoch": 29.857142857142858,
      "grad_norm": 0.25709810853004456,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0309,
      "step": 209
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.20422841608524323,
      "learning_rate": 7.888888888888888e-05,
      "loss": 0.0233,
      "step": 210
    },
    {
      "epoch": 30.142857142857142,
      "grad_norm": 0.19323331117630005,
      "learning_rate": 7.777777777777777e-05,
      "loss": 0.0213,
      "step": 211
    },
    {
      "epoch": 30.285714285714285,
      "grad_norm": 0.20027144253253937,
      "learning_rate": 7.666666666666666e-05,
      "loss": 0.0265,
      "step": 212
    },
    {
      "epoch": 30.428571428571427,
      "grad_norm": 0.17917349934577942,
      "learning_rate": 7.555555555555554e-05,
      "loss": 0.023,
      "step": 213
    },
    {
      "epoch": 30.571428571428573,
      "grad_norm": 0.23719751834869385,
      "learning_rate": 7.444444444444443e-05,
      "loss": 0.0246,
      "step": 214
    },
    {
      "epoch": 30.714285714285715,
      "grad_norm": 0.21555249392986298,
      "learning_rate": 7.333333333333332e-05,
      "loss": 0.0282,
      "step": 215
    },
    {
      "epoch": 30.857142857142858,
      "grad_norm": 0.23207108676433563,
      "learning_rate": 7.22222222222222e-05,
      "loss": 0.0315,
      "step": 216
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.19950051605701447,
      "learning_rate": 7.11111111111111e-05,
      "loss": 0.0258,
      "step": 217
    },
    {
      "epoch": 31.142857142857142,
      "grad_norm": 0.15703529119491577,
      "learning_rate": 7e-05,
      "loss": 0.0228,
      "step": 218
    },
    {
      "epoch": 31.285714285714285,
      "grad_norm": 0.3276136517524719,
      "learning_rate": 6.888888888888888e-05,
      "loss": 0.0244,
      "step": 219
    },
    {
      "epoch": 31.428571428571427,
      "grad_norm": 0.20047414302825928,
      "learning_rate": 6.777777777777777e-05,
      "loss": 0.0263,
      "step": 220
    },
    {
      "epoch": 31.571428571428573,
      "grad_norm": 1.124477744102478,
      "learning_rate": 6.666666666666666e-05,
      "loss": 0.0267,
      "step": 221
    },
    {
      "epoch": 31.714285714285715,
      "grad_norm": 0.92868971824646,
      "learning_rate": 6.555555555555556e-05,
      "loss": 0.0468,
      "step": 222
    },
    {
      "epoch": 31.857142857142858,
      "grad_norm": 0.31165021657943726,
      "learning_rate": 6.444444444444444e-05,
      "loss": 0.0236,
      "step": 223
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.5480073094367981,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.0333,
      "step": 224
    },
    {
      "epoch": 32.142857142857146,
      "grad_norm": 0.26246312260627747,
      "learning_rate": 6.222222222222222e-05,
      "loss": 0.0294,
      "step": 225
    },
    {
      "epoch": 32.285714285714285,
      "grad_norm": 0.41255655884742737,
      "learning_rate": 6.111111111111111e-05,
      "loss": 0.0336,
      "step": 226
    },
    {
      "epoch": 32.42857142857143,
      "grad_norm": 0.2280983030796051,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0214,
      "step": 227
    },
    {
      "epoch": 32.57142857142857,
      "grad_norm": 0.7260520458221436,
      "learning_rate": 5.888888888888888e-05,
      "loss": 0.0329,
      "step": 228
    },
    {
      "epoch": 32.714285714285715,
      "grad_norm": 0.27995702624320984,
      "learning_rate": 5.777777777777777e-05,
      "loss": 0.0254,
      "step": 229
    },
    {
      "epoch": 32.857142857142854,
      "grad_norm": 0.20517145097255707,
      "learning_rate": 5.666666666666666e-05,
      "loss": 0.0258,
      "step": 230
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.5841954350471497,
      "learning_rate": 5.5555555555555545e-05,
      "loss": 0.0341,
      "step": 231
    },
    {
      "epoch": 33.142857142857146,
      "grad_norm": 0.1746654361486435,
      "learning_rate": 5.4444444444444446e-05,
      "loss": 0.025,
      "step": 232
    },
    {
      "epoch": 33.285714285714285,
      "grad_norm": 0.1786973476409912,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0259,
      "step": 233
    },
    {
      "epoch": 33.42857142857143,
      "grad_norm": 0.18522648513317108,
      "learning_rate": 5.222222222222222e-05,
      "loss": 0.0237,
      "step": 234
    },
    {
      "epoch": 33.57142857142857,
      "grad_norm": 0.2773274779319763,
      "learning_rate": 5.111111111111111e-05,
      "loss": 0.0212,
      "step": 235
    },
    {
      "epoch": 33.714285714285715,
      "grad_norm": 0.2005719542503357,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0248,
      "step": 236
    },
    {
      "epoch": 33.857142857142854,
      "grad_norm": 0.20350269973278046,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.026,
      "step": 237
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.20436100661754608,
      "learning_rate": 4.777777777777778e-05,
      "loss": 0.0253,
      "step": 238
    },
    {
      "epoch": 34.142857142857146,
      "grad_norm": 0.28392311930656433,
      "learning_rate": 4.6666666666666665e-05,
      "loss": 0.0219,
      "step": 239
    },
    {
      "epoch": 34.285714285714285,
      "grad_norm": 0.30343034863471985,
      "learning_rate": 4.555555555555555e-05,
      "loss": 0.0278,
      "step": 240
    },
    {
      "epoch": 34.42857142857143,
      "grad_norm": 0.17426952719688416,
      "learning_rate": 4.444444444444444e-05,
      "loss": 0.0221,
      "step": 241
    },
    {
      "epoch": 34.57142857142857,
      "grad_norm": 0.20048090815544128,
      "learning_rate": 4.333333333333333e-05,
      "loss": 0.026,
      "step": 242
    },
    {
      "epoch": 34.714285714285715,
      "grad_norm": 0.21414272487163544,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0254,
      "step": 243
    },
    {
      "epoch": 34.857142857142854,
      "grad_norm": 0.24281620979309082,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0249,
      "step": 244
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.16078902781009674,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.0225,
      "step": 245
    },
    {
      "epoch": 35.142857142857146,
      "grad_norm": 0.4510670006275177,
      "learning_rate": 3.8888888888888884e-05,
      "loss": 0.0214,
      "step": 246
    },
    {
      "epoch": 35.285714285714285,
      "grad_norm": 0.23294629156589508,
      "learning_rate": 3.777777777777777e-05,
      "loss": 0.0287,
      "step": 247
    },
    {
      "epoch": 35.42857142857143,
      "grad_norm": 0.1711610108613968,
      "learning_rate": 3.666666666666666e-05,
      "loss": 0.0253,
      "step": 248
    },
    {
      "epoch": 35.57142857142857,
      "grad_norm": 0.20452380180358887,
      "learning_rate": 3.555555555555555e-05,
      "loss": 0.0252,
      "step": 249
    },
    {
      "epoch": 35.714285714285715,
      "grad_norm": 0.199001282453537,
      "learning_rate": 3.444444444444444e-05,
      "loss": 0.022,
      "step": 250
    },
    {
      "epoch": 35.857142857142854,
      "grad_norm": 0.19277751445770264,
      "learning_rate": 3.333333333333333e-05,
      "loss": 0.0236,
      "step": 251
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.187498077750206,
      "learning_rate": 3.222222222222222e-05,
      "loss": 0.0259,
      "step": 252
    },
    {
      "epoch": 36.142857142857146,
      "grad_norm": 0.19354349374771118,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0273,
      "step": 253
    },
    {
      "epoch": 36.285714285714285,
      "grad_norm": 0.17063525319099426,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0208,
      "step": 254
    },
    {
      "epoch": 36.42857142857143,
      "grad_norm": 0.19611071050167084,
      "learning_rate": 2.8888888888888885e-05,
      "loss": 0.0215,
      "step": 255
    },
    {
      "epoch": 36.57142857142857,
      "grad_norm": 0.1668803095817566,
      "learning_rate": 2.7777777777777772e-05,
      "loss": 0.0203,
      "step": 256
    },
    {
      "epoch": 36.714285714285715,
      "grad_norm": 0.18679891526699066,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0261,
      "step": 257
    },
    {
      "epoch": 36.857142857142854,
      "grad_norm": 0.18383032083511353,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.0241,
      "step": 258
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.3405314087867737,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0327,
      "step": 259
    },
    {
      "epoch": 37.142857142857146,
      "grad_norm": 0.24260462820529938,
      "learning_rate": 2.3333333333333332e-05,
      "loss": 0.0258,
      "step": 260
    },
    {
      "epoch": 37.285714285714285,
      "grad_norm": 0.16929350793361664,
      "learning_rate": 2.222222222222222e-05,
      "loss": 0.0204,
      "step": 261
    },
    {
      "epoch": 37.42857142857143,
      "grad_norm": 0.1627090871334076,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.0233,
      "step": 262
    },
    {
      "epoch": 37.57142857142857,
      "grad_norm": 0.21464985609054565,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0252,
      "step": 263
    },
    {
      "epoch": 37.714285714285715,
      "grad_norm": 0.23557142913341522,
      "learning_rate": 1.8888888888888886e-05,
      "loss": 0.0274,
      "step": 264
    },
    {
      "epoch": 37.857142857142854,
      "grad_norm": 0.15591800212860107,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.0203,
      "step": 265
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.1976712942123413,
      "learning_rate": 1.6666666666666664e-05,
      "loss": 0.0223,
      "step": 266
    },
    {
      "epoch": 38.142857142857146,
      "grad_norm": 0.16047793626785278,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0226,
      "step": 267
    },
    {
      "epoch": 38.285714285714285,
      "grad_norm": 0.20493368804454803,
      "learning_rate": 1.4444444444444442e-05,
      "loss": 0.0273,
      "step": 268
    },
    {
      "epoch": 38.42857142857143,
      "grad_norm": 0.18004454672336578,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0215,
      "step": 269
    },
    {
      "epoch": 38.57142857142857,
      "grad_norm": 0.1710720807313919,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0217,
      "step": 270
    },
    {
      "epoch": 38.714285714285715,
      "grad_norm": 0.18787555396556854,
      "learning_rate": 1.111111111111111e-05,
      "loss": 0.0226,
      "step": 271
    },
    {
      "epoch": 38.857142857142854,
      "grad_norm": 0.20953808724880219,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0276,
      "step": 272
    },
    {
      "epoch": 39.0,
      "grad_norm": 0.21426615118980408,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.0204,
      "step": 273
    },
    {
      "epoch": 39.142857142857146,
      "grad_norm": 0.17735669016838074,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0232,
      "step": 274
    },
    {
      "epoch": 39.285714285714285,
      "grad_norm": 0.20991182327270508,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.025,
      "step": 275
    },
    {
      "epoch": 39.42857142857143,
      "grad_norm": 0.18005642294883728,
      "learning_rate": 5.555555555555555e-06,
      "loss": 0.0213,
      "step": 276
    },
    {
      "epoch": 39.57142857142857,
      "grad_norm": 0.18135222792625427,
      "learning_rate": 4.444444444444444e-06,
      "loss": 0.0224,
      "step": 277
    },
    {
      "epoch": 39.714285714285715,
      "grad_norm": 0.21405084431171417,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0254,
      "step": 278
    },
    {
      "epoch": 39.857142857142854,
      "grad_norm": 0.17209117114543915,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.0216,
      "step": 279
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.21015004813671112,
      "learning_rate": 1.111111111111111e-06,
      "loss": 0.0233,
      "step": 280
    }
  ],
  "logging_steps": 1,
  "max_steps": 280,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8035015143235584.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

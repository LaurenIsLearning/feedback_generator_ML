{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 280,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 9.060221672058105,
      "learning_rate": 0.0,
      "loss": 3.827,
      "step": 1
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 8.706445693969727,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 3.7175,
      "step": 2
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.277660369873047,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 3.6803,
      "step": 3
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 10.324462890625,
      "learning_rate": 8.999999999999999e-05,
      "loss": 3.6434,
      "step": 4
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 7.289487838745117,
      "learning_rate": 0.00011999999999999999,
      "loss": 3.455,
      "step": 5
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 7.706867694854736,
      "learning_rate": 0.00015,
      "loss": 3.0778,
      "step": 6
    },
    {
      "epoch": 1.0,
      "grad_norm": 4.3723649978637695,
      "learning_rate": 0.00017999999999999998,
      "loss": 2.846,
      "step": 7
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 2.8649353981018066,
      "learning_rate": 0.00020999999999999998,
      "loss": 2.6933,
      "step": 8
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 2.1385836601257324,
      "learning_rate": 0.00023999999999999998,
      "loss": 2.3678,
      "step": 9
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 2.5354483127593994,
      "learning_rate": 0.00027,
      "loss": 2.3006,
      "step": 10
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.9368466138839722,
      "learning_rate": 0.0003,
      "loss": 2.0274,
      "step": 11
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.4855766296386719,
      "learning_rate": 0.0002988888888888889,
      "loss": 1.8914,
      "step": 12
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.5824567079544067,
      "learning_rate": 0.0002977777777777777,
      "loss": 1.9127,
      "step": 13
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2481969594955444,
      "learning_rate": 0.00029666666666666665,
      "loss": 1.8528,
      "step": 14
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.0778968334197998,
      "learning_rate": 0.0002955555555555555,
      "loss": 1.7779,
      "step": 15
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 1.1033594608306885,
      "learning_rate": 0.00029444444444444445,
      "loss": 1.6375,
      "step": 16
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 1.4964799880981445,
      "learning_rate": 0.00029333333333333327,
      "loss": 1.8131,
      "step": 17
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.545177936553955,
      "learning_rate": 0.0002922222222222222,
      "loss": 1.6205,
      "step": 18
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 1.7548775672912598,
      "learning_rate": 0.0002911111111111111,
      "loss": 1.6405,
      "step": 19
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 1.7365413904190063,
      "learning_rate": 0.00029,
      "loss": 1.7401,
      "step": 20
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.4090080261230469,
      "learning_rate": 0.0002888888888888888,
      "loss": 1.6818,
      "step": 21
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 1.3564141988754272,
      "learning_rate": 0.00028777777777777775,
      "loss": 1.4132,
      "step": 22
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 1.5305784940719604,
      "learning_rate": 0.0002866666666666667,
      "loss": 1.4358,
      "step": 23
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.266923189163208,
      "learning_rate": 0.00028555555555555555,
      "loss": 1.0622,
      "step": 24
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 26.235363006591797,
      "learning_rate": 0.0002844444444444444,
      "loss": 1.408,
      "step": 25
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 4.44404935836792,
      "learning_rate": 0.0002833333333333333,
      "loss": 1.4042,
      "step": 26
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 2.4135820865631104,
      "learning_rate": 0.00028222222222222223,
      "loss": 1.3369,
      "step": 27
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.8433061838150024,
      "learning_rate": 0.0002811111111111111,
      "loss": 1.3666,
      "step": 28
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 1.4802948236465454,
      "learning_rate": 0.00028,
      "loss": 1.1343,
      "step": 29
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 1.6634095907211304,
      "learning_rate": 0.00027888888888888885,
      "loss": 1.0148,
      "step": 30
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 1.8782000541687012,
      "learning_rate": 0.0002777777777777778,
      "loss": 1.1701,
      "step": 31
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 1.5968724489212036,
      "learning_rate": 0.00027666666666666665,
      "loss": 1.0069,
      "step": 32
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 1.4096759557724,
      "learning_rate": 0.0002755555555555555,
      "loss": 0.8326,
      "step": 33
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 1.5806338787078857,
      "learning_rate": 0.00027444444444444445,
      "loss": 1.1167,
      "step": 34
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.047886610031128,
      "learning_rate": 0.00027333333333333333,
      "loss": 0.9659,
      "step": 35
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 1.678010106086731,
      "learning_rate": 0.0002722222222222222,
      "loss": 0.8093,
      "step": 36
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 1.923966646194458,
      "learning_rate": 0.0002711111111111111,
      "loss": 0.641,
      "step": 37
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 2.037442684173584,
      "learning_rate": 0.00027,
      "loss": 0.7118,
      "step": 38
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 1.9748033285140991,
      "learning_rate": 0.0002688888888888889,
      "loss": 0.6444,
      "step": 39
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 1.5874303579330444,
      "learning_rate": 0.00026777777777777775,
      "loss": 0.7574,
      "step": 40
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 1.8460960388183594,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.7391,
      "step": 41
    },
    {
      "epoch": 6.0,
      "grad_norm": 2.008730173110962,
      "learning_rate": 0.00026555555555555555,
      "loss": 0.8025,
      "step": 42
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 1.7003777027130127,
      "learning_rate": 0.00026444444444444443,
      "loss": 0.5421,
      "step": 43
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 1.4507251977920532,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.451,
      "step": 44
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 1.6141974925994873,
      "learning_rate": 0.00026222222222222223,
      "loss": 0.5752,
      "step": 45
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 2.09505558013916,
      "learning_rate": 0.0002611111111111111,
      "loss": 0.3747,
      "step": 46
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 2.472043991088867,
      "learning_rate": 0.00026,
      "loss": 0.5506,
      "step": 47
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 2.4234724044799805,
      "learning_rate": 0.00025888888888888885,
      "loss": 0.5132,
      "step": 48
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.9598593711853027,
      "learning_rate": 0.0002577777777777778,
      "loss": 0.4313,
      "step": 49
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 1.9379093647003174,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.406,
      "step": 50
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 1.5200834274291992,
      "learning_rate": 0.00025555555555555553,
      "loss": 0.2989,
      "step": 51
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 1.6812918186187744,
      "learning_rate": 0.0002544444444444444,
      "loss": 0.3064,
      "step": 52
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 1.73206627368927,
      "learning_rate": 0.00025333333333333333,
      "loss": 0.2535,
      "step": 53
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 1.8128607273101807,
      "learning_rate": 0.0002522222222222222,
      "loss": 0.302,
      "step": 54
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 2.3304262161254883,
      "learning_rate": 0.0002511111111111111,
      "loss": 0.2974,
      "step": 55
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.4523913860321045,
      "learning_rate": 0.00025,
      "loss": 0.2809,
      "step": 56
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 1.230902075767517,
      "learning_rate": 0.0002488888888888889,
      "loss": 0.2069,
      "step": 57
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 2.311758518218994,
      "learning_rate": 0.00024777777777777775,
      "loss": 0.2157,
      "step": 58
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 2.88464093208313,
      "learning_rate": 0.0002466666666666666,
      "loss": 0.2877,
      "step": 59
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 1.5321897268295288,
      "learning_rate": 0.00024555555555555556,
      "loss": 0.183,
      "step": 60
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 1.912527322769165,
      "learning_rate": 0.00024444444444444443,
      "loss": 0.2051,
      "step": 61
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 1.7145652770996094,
      "learning_rate": 0.0002433333333333333,
      "loss": 0.2095,
      "step": 62
    },
    {
      "epoch": 9.0,
      "grad_norm": 1.599409818649292,
      "learning_rate": 0.0002422222222222222,
      "loss": 0.1888,
      "step": 63
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 1.151817798614502,
      "learning_rate": 0.00024111111111111108,
      "loss": 0.1478,
      "step": 64
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 1.287758469581604,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.1419,
      "step": 65
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 1.4211565256118774,
      "learning_rate": 0.00023888888888888885,
      "loss": 0.1244,
      "step": 66
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 1.6100236177444458,
      "learning_rate": 0.00023777777777777775,
      "loss": 0.1385,
      "step": 67
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 1.0811911821365356,
      "learning_rate": 0.00023666666666666663,
      "loss": 0.1007,
      "step": 68
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 1.5843204259872437,
      "learning_rate": 0.00023555555555555553,
      "loss": 0.1646,
      "step": 69
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.1694976091384888,
      "learning_rate": 0.0002344444444444444,
      "loss": 0.0978,
      "step": 70
    },
    {
      "epoch": 10.142857142857142,
      "grad_norm": 1.1865042448043823,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.0911,
      "step": 71
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 0.945704996585846,
      "learning_rate": 0.00023222222222222218,
      "loss": 0.0859,
      "step": 72
    },
    {
      "epoch": 10.428571428571429,
      "grad_norm": 1.1319488286972046,
      "learning_rate": 0.00023111111111111108,
      "loss": 0.0812,
      "step": 73
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 1.514817237854004,
      "learning_rate": 0.00023,
      "loss": 0.1017,
      "step": 74
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 1.3908398151397705,
      "learning_rate": 0.00022888888888888885,
      "loss": 0.0847,
      "step": 75
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 1.559133529663086,
      "learning_rate": 0.00022777777777777778,
      "loss": 0.1046,
      "step": 76
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.983241617679596,
      "learning_rate": 0.00022666666666666663,
      "loss": 0.075,
      "step": 77
    },
    {
      "epoch": 11.142857142857142,
      "grad_norm": 0.8990335464477539,
      "learning_rate": 0.00022555555555555556,
      "loss": 0.0716,
      "step": 78
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 1.00669527053833,
      "learning_rate": 0.0002244444444444444,
      "loss": 0.0667,
      "step": 79
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 1.2162362337112427,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.1043,
      "step": 80
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 0.969106912612915,
      "learning_rate": 0.00022222222222222218,
      "loss": 0.0623,
      "step": 81
    },
    {
      "epoch": 11.714285714285714,
      "grad_norm": 1.004837989807129,
      "learning_rate": 0.0002211111111111111,
      "loss": 0.0744,
      "step": 82
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 0.8498032689094543,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.054,
      "step": 83
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.1557409763336182,
      "learning_rate": 0.00021888888888888888,
      "loss": 0.0756,
      "step": 84
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 2.0329723358154297,
      "learning_rate": 0.00021777777777777778,
      "loss": 0.1395,
      "step": 85
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 1.1733304262161255,
      "learning_rate": 0.00021666666666666666,
      "loss": 0.0702,
      "step": 86
    },
    {
      "epoch": 12.428571428571429,
      "grad_norm": 1.0532366037368774,
      "learning_rate": 0.00021555555555555556,
      "loss": 0.0742,
      "step": 87
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 0.680464506149292,
      "learning_rate": 0.00021444444444444443,
      "loss": 0.0505,
      "step": 88
    },
    {
      "epoch": 12.714285714285714,
      "grad_norm": 0.993531346321106,
      "learning_rate": 0.00021333333333333333,
      "loss": 0.0688,
      "step": 89
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 0.8773494958877563,
      "learning_rate": 0.0002122222222222222,
      "loss": 0.0633,
      "step": 90
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.6435388922691345,
      "learning_rate": 0.0002111111111111111,
      "loss": 0.0535,
      "step": 91
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 0.7743216753005981,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.0469,
      "step": 92
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 0.6203442215919495,
      "learning_rate": 0.00020888888888888888,
      "loss": 0.0402,
      "step": 93
    },
    {
      "epoch": 13.428571428571429,
      "grad_norm": 0.6634601354598999,
      "learning_rate": 0.00020777777777777776,
      "loss": 0.0431,
      "step": 94
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 0.9124431610107422,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.058,
      "step": 95
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 0.6272336840629578,
      "learning_rate": 0.00020555555555555556,
      "loss": 0.0406,
      "step": 96
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 0.8317899703979492,
      "learning_rate": 0.00020444444444444443,
      "loss": 0.0424,
      "step": 97
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.8208177089691162,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.0417,
      "step": 98
    },
    {
      "epoch": 14.142857142857142,
      "grad_norm": 0.844214916229248,
      "learning_rate": 0.0002022222222222222,
      "loss": 0.0393,
      "step": 99
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 0.49311843514442444,
      "learning_rate": 0.0002011111111111111,
      "loss": 0.0296,
      "step": 100
    },
    {
      "epoch": 14.428571428571429,
      "grad_norm": 0.5570082664489746,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.0338,
      "step": 101
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 0.6591638922691345,
      "learning_rate": 0.00019888888888888888,
      "loss": 0.0357,
      "step": 102
    },
    {
      "epoch": 14.714285714285714,
      "grad_norm": 0.637643039226532,
      "learning_rate": 0.00019777777777777776,
      "loss": 0.0323,
      "step": 103
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 0.7890963554382324,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.0357,
      "step": 104
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.5757165551185608,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.0346,
      "step": 105
    },
    {
      "epoch": 15.142857142857142,
      "grad_norm": 0.43478891253471375,
      "learning_rate": 0.00019444444444444443,
      "loss": 0.0254,
      "step": 106
    },
    {
      "epoch": 15.285714285714286,
      "grad_norm": 0.8521354794502258,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.037,
      "step": 107
    },
    {
      "epoch": 15.428571428571429,
      "grad_norm": 0.5419595241546631,
      "learning_rate": 0.0001922222222222222,
      "loss": 0.0338,
      "step": 108
    },
    {
      "epoch": 15.571428571428571,
      "grad_norm": 0.6270205974578857,
      "learning_rate": 0.0001911111111111111,
      "loss": 0.0305,
      "step": 109
    },
    {
      "epoch": 15.714285714285714,
      "grad_norm": 0.6251391172409058,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0359,
      "step": 110
    },
    {
      "epoch": 15.857142857142858,
      "grad_norm": 0.6699425578117371,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.0341,
      "step": 111
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.6675704717636108,
      "learning_rate": 0.00018777777777777776,
      "loss": 0.0402,
      "step": 112
    },
    {
      "epoch": 16.142857142857142,
      "grad_norm": 0.48908331990242004,
      "learning_rate": 0.00018666666666666666,
      "loss": 0.0277,
      "step": 113
    },
    {
      "epoch": 16.285714285714285,
      "grad_norm": 0.3525699973106384,
      "learning_rate": 0.00018555555555555553,
      "loss": 0.0247,
      "step": 114
    },
    {
      "epoch": 16.428571428571427,
      "grad_norm": 0.8674250841140747,
      "learning_rate": 0.00018444444444444443,
      "loss": 0.0377,
      "step": 115
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 0.6752074360847473,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.032,
      "step": 116
    },
    {
      "epoch": 16.714285714285715,
      "grad_norm": 0.8183134198188782,
      "learning_rate": 0.0001822222222222222,
      "loss": 0.0351,
      "step": 117
    },
    {
      "epoch": 16.857142857142858,
      "grad_norm": 0.7276458740234375,
      "learning_rate": 0.0001811111111111111,
      "loss": 0.0414,
      "step": 118
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.46744394302368164,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.028,
      "step": 119
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 0.24159540235996246,
      "learning_rate": 0.00017888888888888889,
      "loss": 0.0226,
      "step": 120
    },
    {
      "epoch": 17.285714285714285,
      "grad_norm": 0.5058221817016602,
      "learning_rate": 0.00017777777777777776,
      "loss": 0.028,
      "step": 121
    },
    {
      "epoch": 17.428571428571427,
      "grad_norm": 0.4654800593852997,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.0231,
      "step": 122
    },
    {
      "epoch": 17.571428571428573,
      "grad_norm": 0.6578453779220581,
      "learning_rate": 0.00017555555555555553,
      "loss": 0.034,
      "step": 123
    },
    {
      "epoch": 17.714285714285715,
      "grad_norm": 0.4646209478378296,
      "learning_rate": 0.00017444444444444444,
      "loss": 0.0269,
      "step": 124
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 0.5369648933410645,
      "learning_rate": 0.0001733333333333333,
      "loss": 0.0279,
      "step": 125
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.5851529240608215,
      "learning_rate": 0.0001722222222222222,
      "loss": 0.0324,
      "step": 126
    },
    {
      "epoch": 18.142857142857142,
      "grad_norm": 0.37097734212875366,
      "learning_rate": 0.0001711111111111111,
      "loss": 0.0244,
      "step": 127
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 0.4449259340763092,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0244,
      "step": 128
    },
    {
      "epoch": 18.428571428571427,
      "grad_norm": 0.23930667340755463,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.0212,
      "step": 129
    },
    {
      "epoch": 18.571428571428573,
      "grad_norm": 0.29213032126426697,
      "learning_rate": 0.00016777777777777776,
      "loss": 0.022,
      "step": 130
    },
    {
      "epoch": 18.714285714285715,
      "grad_norm": 0.38990405201911926,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.0235,
      "step": 131
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 0.6460464596748352,
      "learning_rate": 0.00016555555555555554,
      "loss": 0.034,
      "step": 132
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.47954997420310974,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.0261,
      "step": 133
    },
    {
      "epoch": 19.142857142857142,
      "grad_norm": 0.38505321741104126,
      "learning_rate": 0.0001633333333333333,
      "loss": 0.0209,
      "step": 134
    },
    {
      "epoch": 19.285714285714285,
      "grad_norm": 0.30188286304473877,
      "learning_rate": 0.0001622222222222222,
      "loss": 0.0204,
      "step": 135
    },
    {
      "epoch": 19.428571428571427,
      "grad_norm": 0.3507694900035858,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.0214,
      "step": 136
    },
    {
      "epoch": 19.571428571428573,
      "grad_norm": 0.6252952814102173,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.024,
      "step": 137
    },
    {
      "epoch": 19.714285714285715,
      "grad_norm": 0.3274379074573517,
      "learning_rate": 0.0001588888888888889,
      "loss": 0.0198,
      "step": 138
    },
    {
      "epoch": 19.857142857142858,
      "grad_norm": 0.29274237155914307,
      "learning_rate": 0.00015777777777777776,
      "loss": 0.0218,
      "step": 139
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.27970242500305176,
      "learning_rate": 0.00015666666666666666,
      "loss": 0.0225,
      "step": 140
    },
    {
      "epoch": 20.142857142857142,
      "grad_norm": 0.17358721792697906,
      "learning_rate": 0.00015555555555555554,
      "loss": 0.0165,
      "step": 141
    },
    {
      "epoch": 20.285714285714285,
      "grad_norm": 0.526856541633606,
      "learning_rate": 0.00015444444444444444,
      "loss": 0.0209,
      "step": 142
    },
    {
      "epoch": 20.428571428571427,
      "grad_norm": 0.19060851633548737,
      "learning_rate": 0.0001533333333333333,
      "loss": 0.0188,
      "step": 143
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 0.438305139541626,
      "learning_rate": 0.0001522222222222222,
      "loss": 0.0236,
      "step": 144
    },
    {
      "epoch": 20.714285714285715,
      "grad_norm": 0.8275175094604492,
      "learning_rate": 0.00015111111111111109,
      "loss": 0.0248,
      "step": 145
    },
    {
      "epoch": 20.857142857142858,
      "grad_norm": 0.4691658020019531,
      "learning_rate": 0.00015,
      "loss": 0.0237,
      "step": 146
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.3064434230327606,
      "learning_rate": 0.00014888888888888886,
      "loss": 0.0229,
      "step": 147
    },
    {
      "epoch": 21.142857142857142,
      "grad_norm": 0.1693880409002304,
      "learning_rate": 0.00014777777777777776,
      "loss": 0.0192,
      "step": 148
    },
    {
      "epoch": 21.285714285714285,
      "grad_norm": 0.653803825378418,
      "learning_rate": 0.00014666666666666664,
      "loss": 0.0258,
      "step": 149
    },
    {
      "epoch": 21.428571428571427,
      "grad_norm": 0.49453824758529663,
      "learning_rate": 0.00014555555555555554,
      "loss": 0.0266,
      "step": 150
    },
    {
      "epoch": 21.571428571428573,
      "grad_norm": 0.4154583513736725,
      "learning_rate": 0.0001444444444444444,
      "loss": 0.0232,
      "step": 151
    },
    {
      "epoch": 21.714285714285715,
      "grad_norm": 0.38241392374038696,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.0218,
      "step": 152
    },
    {
      "epoch": 21.857142857142858,
      "grad_norm": 0.8324690461158752,
      "learning_rate": 0.0001422222222222222,
      "loss": 0.0329,
      "step": 153
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.6143761873245239,
      "learning_rate": 0.00014111111111111111,
      "loss": 0.0248,
      "step": 154
    },
    {
      "epoch": 22.142857142857142,
      "grad_norm": 0.3460165858268738,
      "learning_rate": 0.00014,
      "loss": 0.0206,
      "step": 155
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 0.18434244394302368,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.0188,
      "step": 156
    },
    {
      "epoch": 22.428571428571427,
      "grad_norm": 0.2102925330400467,
      "learning_rate": 0.00013777777777777776,
      "loss": 0.02,
      "step": 157
    },
    {
      "epoch": 22.571428571428573,
      "grad_norm": 0.48083195090293884,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.0204,
      "step": 158
    },
    {
      "epoch": 22.714285714285715,
      "grad_norm": 0.35299476981163025,
      "learning_rate": 0.00013555555555555554,
      "loss": 0.0217,
      "step": 159
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 0.28860414028167725,
      "learning_rate": 0.00013444444444444444,
      "loss": 0.0243,
      "step": 160
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.37708958983421326,
      "learning_rate": 0.0001333333333333333,
      "loss": 0.0237,
      "step": 161
    },
    {
      "epoch": 23.142857142857142,
      "grad_norm": 0.3790399432182312,
      "learning_rate": 0.00013222222222222221,
      "loss": 0.0201,
      "step": 162
    },
    {
      "epoch": 23.285714285714285,
      "grad_norm": 0.5720332264900208,
      "learning_rate": 0.00013111111111111111,
      "loss": 0.0228,
      "step": 163
    },
    {
      "epoch": 23.428571428571427,
      "grad_norm": 0.2287929803133011,
      "learning_rate": 0.00013,
      "loss": 0.0208,
      "step": 164
    },
    {
      "epoch": 23.571428571428573,
      "grad_norm": 0.49157312512397766,
      "learning_rate": 0.0001288888888888889,
      "loss": 0.0218,
      "step": 165
    },
    {
      "epoch": 23.714285714285715,
      "grad_norm": 0.5043801665306091,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.0181,
      "step": 166
    },
    {
      "epoch": 23.857142857142858,
      "grad_norm": 0.5864675641059875,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.0324,
      "step": 167
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.4935304522514343,
      "learning_rate": 0.00012555555555555554,
      "loss": 0.0213,
      "step": 168
    },
    {
      "epoch": 24.142857142857142,
      "grad_norm": 0.15866312384605408,
      "learning_rate": 0.00012444444444444444,
      "loss": 0.0157,
      "step": 169
    },
    {
      "epoch": 24.285714285714285,
      "grad_norm": 0.1744721382856369,
      "learning_rate": 0.0001233333333333333,
      "loss": 0.017,
      "step": 170
    },
    {
      "epoch": 24.428571428571427,
      "grad_norm": 0.14525024592876434,
      "learning_rate": 0.00012222222222222221,
      "loss": 0.0184,
      "step": 171
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 0.4097215533256531,
      "learning_rate": 0.0001211111111111111,
      "loss": 0.021,
      "step": 172
    },
    {
      "epoch": 24.714285714285715,
      "grad_norm": 0.659022331237793,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0269,
      "step": 173
    },
    {
      "epoch": 24.857142857142858,
      "grad_norm": 0.19668802618980408,
      "learning_rate": 0.00011888888888888888,
      "loss": 0.0187,
      "step": 174
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.45954883098602295,
      "learning_rate": 0.00011777777777777776,
      "loss": 0.02,
      "step": 175
    },
    {
      "epoch": 25.142857142857142,
      "grad_norm": 0.6745473146438599,
      "learning_rate": 0.00011666666666666665,
      "loss": 0.0273,
      "step": 176
    },
    {
      "epoch": 25.285714285714285,
      "grad_norm": 0.163900688290596,
      "learning_rate": 0.00011555555555555554,
      "loss": 0.0174,
      "step": 177
    },
    {
      "epoch": 25.428571428571427,
      "grad_norm": 0.328676700592041,
      "learning_rate": 0.00011444444444444443,
      "loss": 0.0181,
      "step": 178
    },
    {
      "epoch": 25.571428571428573,
      "grad_norm": 0.38849520683288574,
      "learning_rate": 0.00011333333333333331,
      "loss": 0.02,
      "step": 179
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 0.14427867531776428,
      "learning_rate": 0.0001122222222222222,
      "loss": 0.0175,
      "step": 180
    },
    {
      "epoch": 25.857142857142858,
      "grad_norm": 0.21178415417671204,
      "learning_rate": 0.00011111111111111109,
      "loss": 0.0185,
      "step": 181
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.3940684497356415,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.023,
      "step": 182
    },
    {
      "epoch": 26.142857142857142,
      "grad_norm": 0.19456860423088074,
      "learning_rate": 0.00010888888888888889,
      "loss": 0.018,
      "step": 183
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 0.13986773788928986,
      "learning_rate": 0.00010777777777777778,
      "loss": 0.0168,
      "step": 184
    },
    {
      "epoch": 26.428571428571427,
      "grad_norm": 0.1757446527481079,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.0158,
      "step": 185
    },
    {
      "epoch": 26.571428571428573,
      "grad_norm": 0.14533179998397827,
      "learning_rate": 0.00010555555555555555,
      "loss": 0.0165,
      "step": 186
    },
    {
      "epoch": 26.714285714285715,
      "grad_norm": 0.18321187794208527,
      "learning_rate": 0.00010444444444444444,
      "loss": 0.0185,
      "step": 187
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 0.18798646330833435,
      "learning_rate": 0.00010333333333333333,
      "loss": 0.0193,
      "step": 188
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.36441436409950256,
      "learning_rate": 0.00010222222222222222,
      "loss": 0.0216,
      "step": 189
    },
    {
      "epoch": 27.142857142857142,
      "grad_norm": 0.19366583228111267,
      "learning_rate": 0.0001011111111111111,
      "loss": 0.0192,
      "step": 190
    },
    {
      "epoch": 27.285714285714285,
      "grad_norm": 0.17170575261116028,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.0186,
      "step": 191
    },
    {
      "epoch": 27.428571428571427,
      "grad_norm": 0.49931424856185913,
      "learning_rate": 9.888888888888888e-05,
      "loss": 0.0178,
      "step": 192
    },
    {
      "epoch": 27.571428571428573,
      "grad_norm": 0.507728099822998,
      "learning_rate": 9.777777777777778e-05,
      "loss": 0.0173,
      "step": 193
    },
    {
      "epoch": 27.714285714285715,
      "grad_norm": 0.16198736429214478,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.018,
      "step": 194
    },
    {
      "epoch": 27.857142857142858,
      "grad_norm": 0.25509753823280334,
      "learning_rate": 9.555555555555555e-05,
      "loss": 0.0218,
      "step": 195
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.20093367993831635,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.0189,
      "step": 196
    },
    {
      "epoch": 28.142857142857142,
      "grad_norm": 0.2104811668395996,
      "learning_rate": 9.333333333333333e-05,
      "loss": 0.0197,
      "step": 197
    },
    {
      "epoch": 28.285714285714285,
      "grad_norm": 0.19346792995929718,
      "learning_rate": 9.222222222222222e-05,
      "loss": 0.0174,
      "step": 198
    },
    {
      "epoch": 28.428571428571427,
      "grad_norm": 0.14891579747200012,
      "learning_rate": 9.11111111111111e-05,
      "loss": 0.0172,
      "step": 199
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 0.16576670110225677,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0179,
      "step": 200
    },
    {
      "epoch": 28.714285714285715,
      "grad_norm": 0.15557321906089783,
      "learning_rate": 8.888888888888888e-05,
      "loss": 0.0163,
      "step": 201
    },
    {
      "epoch": 28.857142857142858,
      "grad_norm": 0.1433691680431366,
      "learning_rate": 8.777777777777777e-05,
      "loss": 0.016,
      "step": 202
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.15210720896720886,
      "learning_rate": 8.666666666666665e-05,
      "loss": 0.0191,
      "step": 203
    },
    {
      "epoch": 29.142857142857142,
      "grad_norm": 0.3026500344276428,
      "learning_rate": 8.555555555555556e-05,
      "loss": 0.0187,
      "step": 204
    },
    {
      "epoch": 29.285714285714285,
      "grad_norm": 0.13119669258594513,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.0167,
      "step": 205
    },
    {
      "epoch": 29.428571428571427,
      "grad_norm": 0.1481207311153412,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.0175,
      "step": 206
    },
    {
      "epoch": 29.571428571428573,
      "grad_norm": 0.12688712775707245,
      "learning_rate": 8.222222222222222e-05,
      "loss": 0.0165,
      "step": 207
    },
    {
      "epoch": 29.714285714285715,
      "grad_norm": 0.1361222118139267,
      "learning_rate": 8.11111111111111e-05,
      "loss": 0.0175,
      "step": 208
    },
    {
      "epoch": 29.857142857142858,
      "grad_norm": 0.23724786937236786,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0193,
      "step": 209
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.14588114619255066,
      "learning_rate": 7.888888888888888e-05,
      "loss": 0.0157,
      "step": 210
    },
    {
      "epoch": 30.142857142857142,
      "grad_norm": 0.15110552310943604,
      "learning_rate": 7.777777777777777e-05,
      "loss": 0.0148,
      "step": 211
    },
    {
      "epoch": 30.285714285714285,
      "grad_norm": 0.14071212708950043,
      "learning_rate": 7.666666666666666e-05,
      "loss": 0.0176,
      "step": 212
    },
    {
      "epoch": 30.428571428571427,
      "grad_norm": 0.12717582285404205,
      "learning_rate": 7.555555555555554e-05,
      "loss": 0.0158,
      "step": 213
    },
    {
      "epoch": 30.571428571428573,
      "grad_norm": 0.1495407670736313,
      "learning_rate": 7.444444444444443e-05,
      "loss": 0.0162,
      "step": 214
    },
    {
      "epoch": 30.714285714285715,
      "grad_norm": 0.14614237844944,
      "learning_rate": 7.333333333333332e-05,
      "loss": 0.0179,
      "step": 215
    },
    {
      "epoch": 30.857142857142858,
      "grad_norm": 0.17894361913204193,
      "learning_rate": 7.22222222222222e-05,
      "loss": 0.02,
      "step": 216
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.16104716062545776,
      "learning_rate": 7.11111111111111e-05,
      "loss": 0.0174,
      "step": 217
    },
    {
      "epoch": 31.142857142857142,
      "grad_norm": 0.12131049484014511,
      "learning_rate": 7e-05,
      "loss": 0.0159,
      "step": 218
    },
    {
      "epoch": 31.285714285714285,
      "grad_norm": 0.1274307370185852,
      "learning_rate": 6.888888888888888e-05,
      "loss": 0.0155,
      "step": 219
    },
    {
      "epoch": 31.428571428571427,
      "grad_norm": 0.26868936419487,
      "learning_rate": 6.777777777777777e-05,
      "loss": 0.0178,
      "step": 220
    },
    {
      "epoch": 31.571428571428573,
      "grad_norm": 0.12515409290790558,
      "learning_rate": 6.666666666666666e-05,
      "loss": 0.0157,
      "step": 221
    },
    {
      "epoch": 31.714285714285715,
      "grad_norm": 0.8513413071632385,
      "learning_rate": 6.555555555555556e-05,
      "loss": 0.0314,
      "step": 222
    },
    {
      "epoch": 31.857142857142858,
      "grad_norm": 0.16766749322414398,
      "learning_rate": 6.444444444444444e-05,
      "loss": 0.0158,
      "step": 223
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.40831002593040466,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.0228,
      "step": 224
    },
    {
      "epoch": 32.142857142857146,
      "grad_norm": 0.1750102937221527,
      "learning_rate": 6.222222222222222e-05,
      "loss": 0.0181,
      "step": 225
    },
    {
      "epoch": 32.285714285714285,
      "grad_norm": 0.22963182628154755,
      "learning_rate": 6.111111111111111e-05,
      "loss": 0.0214,
      "step": 226
    },
    {
      "epoch": 32.42857142857143,
      "grad_norm": 0.1426687240600586,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0148,
      "step": 227
    },
    {
      "epoch": 32.57142857142857,
      "grad_norm": 0.5025299787521362,
      "learning_rate": 5.888888888888888e-05,
      "loss": 0.0196,
      "step": 228
    },
    {
      "epoch": 32.714285714285715,
      "grad_norm": 0.13065825402736664,
      "learning_rate": 5.777777777777777e-05,
      "loss": 0.0171,
      "step": 229
    },
    {
      "epoch": 32.857142857142854,
      "grad_norm": 0.15445053577423096,
      "learning_rate": 5.666666666666666e-05,
      "loss": 0.0171,
      "step": 230
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.17970576882362366,
      "learning_rate": 5.5555555555555545e-05,
      "loss": 0.0197,
      "step": 231
    },
    {
      "epoch": 33.142857142857146,
      "grad_norm": 0.12432007491588593,
      "learning_rate": 5.4444444444444446e-05,
      "loss": 0.0167,
      "step": 232
    },
    {
      "epoch": 33.285714285714285,
      "grad_norm": 0.11719273030757904,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0168,
      "step": 233
    },
    {
      "epoch": 33.42857142857143,
      "grad_norm": 0.12820377945899963,
      "learning_rate": 5.222222222222222e-05,
      "loss": 0.0164,
      "step": 234
    },
    {
      "epoch": 33.57142857142857,
      "grad_norm": 0.16849303245544434,
      "learning_rate": 5.111111111111111e-05,
      "loss": 0.0143,
      "step": 235
    },
    {
      "epoch": 33.714285714285715,
      "grad_norm": 0.14365439116954803,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0165,
      "step": 236
    },
    {
      "epoch": 33.857142857142854,
      "grad_norm": 0.45059850811958313,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.0183,
      "step": 237
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.14804714918136597,
      "learning_rate": 4.777777777777778e-05,
      "loss": 0.0169,
      "step": 238
    },
    {
      "epoch": 34.142857142857146,
      "grad_norm": 0.108585424721241,
      "learning_rate": 4.6666666666666665e-05,
      "loss": 0.0144,
      "step": 239
    },
    {
      "epoch": 34.285714285714285,
      "grad_norm": 0.13501811027526855,
      "learning_rate": 4.555555555555555e-05,
      "loss": 0.0175,
      "step": 240
    },
    {
      "epoch": 34.42857142857143,
      "grad_norm": 0.15965686738491058,
      "learning_rate": 4.444444444444444e-05,
      "loss": 0.0152,
      "step": 241
    },
    {
      "epoch": 34.57142857142857,
      "grad_norm": 0.14227601885795593,
      "learning_rate": 4.333333333333333e-05,
      "loss": 0.0169,
      "step": 242
    },
    {
      "epoch": 34.714285714285715,
      "grad_norm": 0.18061666190624237,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0171,
      "step": 243
    },
    {
      "epoch": 34.857142857142854,
      "grad_norm": 0.11896906048059464,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.0163,
      "step": 244
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.12080546468496323,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.0155,
      "step": 245
    },
    {
      "epoch": 35.142857142857146,
      "grad_norm": 0.26864510774612427,
      "learning_rate": 3.8888888888888884e-05,
      "loss": 0.0148,
      "step": 246
    },
    {
      "epoch": 35.285714285714285,
      "grad_norm": 0.16225780546665192,
      "learning_rate": 3.777777777777777e-05,
      "loss": 0.0184,
      "step": 247
    },
    {
      "epoch": 35.42857142857143,
      "grad_norm": 0.12507247924804688,
      "learning_rate": 3.666666666666666e-05,
      "loss": 0.0165,
      "step": 248
    },
    {
      "epoch": 35.57142857142857,
      "grad_norm": 0.14985264837741852,
      "learning_rate": 3.555555555555555e-05,
      "loss": 0.0169,
      "step": 249
    },
    {
      "epoch": 35.714285714285715,
      "grad_norm": 0.11807241290807724,
      "learning_rate": 3.444444444444444e-05,
      "loss": 0.015,
      "step": 250
    },
    {
      "epoch": 35.857142857142854,
      "grad_norm": 0.14353157579898834,
      "learning_rate": 3.333333333333333e-05,
      "loss": 0.0159,
      "step": 251
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.1367788314819336,
      "learning_rate": 3.222222222222222e-05,
      "loss": 0.0168,
      "step": 252
    },
    {
      "epoch": 36.142857142857146,
      "grad_norm": 0.13157880306243896,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0174,
      "step": 253
    },
    {
      "epoch": 36.285714285714285,
      "grad_norm": 0.13588900864124298,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0145,
      "step": 254
    },
    {
      "epoch": 36.42857142857143,
      "grad_norm": 0.13860200345516205,
      "learning_rate": 2.8888888888888885e-05,
      "loss": 0.0153,
      "step": 255
    },
    {
      "epoch": 36.57142857142857,
      "grad_norm": 0.13027675449848175,
      "learning_rate": 2.7777777777777772e-05,
      "loss": 0.0141,
      "step": 256
    },
    {
      "epoch": 36.714285714285715,
      "grad_norm": 0.38671255111694336,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0176,
      "step": 257
    },
    {
      "epoch": 36.857142857142854,
      "grad_norm": 0.13420914113521576,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 0.0164,
      "step": 258
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.23055091500282288,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0203,
      "step": 259
    },
    {
      "epoch": 37.142857142857146,
      "grad_norm": 0.15575410425662994,
      "learning_rate": 2.3333333333333332e-05,
      "loss": 0.0166,
      "step": 260
    },
    {
      "epoch": 37.285714285714285,
      "grad_norm": 0.13488902151584625,
      "learning_rate": 2.222222222222222e-05,
      "loss": 0.0144,
      "step": 261
    },
    {
      "epoch": 37.42857142857143,
      "grad_norm": 0.1220642700791359,
      "learning_rate": 2.111111111111111e-05,
      "loss": 0.0156,
      "step": 262
    },
    {
      "epoch": 37.57142857142857,
      "grad_norm": 0.13215193152427673,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0163,
      "step": 263
    },
    {
      "epoch": 37.714285714285715,
      "grad_norm": 0.17471681535243988,
      "learning_rate": 1.8888888888888886e-05,
      "loss": 0.018,
      "step": 264
    },
    {
      "epoch": 37.857142857142854,
      "grad_norm": 0.11893820762634277,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.0143,
      "step": 265
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.1524103581905365,
      "learning_rate": 1.6666666666666664e-05,
      "loss": 0.0157,
      "step": 266
    },
    {
      "epoch": 38.142857142857146,
      "grad_norm": 0.1138630360364914,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0151,
      "step": 267
    },
    {
      "epoch": 38.285714285714285,
      "grad_norm": 0.15141059458255768,
      "learning_rate": 1.4444444444444442e-05,
      "loss": 0.0177,
      "step": 268
    },
    {
      "epoch": 38.42857142857143,
      "grad_norm": 0.12111607193946838,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0144,
      "step": 269
    },
    {
      "epoch": 38.57142857142857,
      "grad_norm": 0.13457152247428894,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 0.0154,
      "step": 270
    },
    {
      "epoch": 38.714285714285715,
      "grad_norm": 0.13708442449569702,
      "learning_rate": 1.111111111111111e-05,
      "loss": 0.0153,
      "step": 271
    },
    {
      "epoch": 38.857142857142854,
      "grad_norm": 0.14452534914016724,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0177,
      "step": 272
    },
    {
      "epoch": 39.0,
      "grad_norm": 0.1434759497642517,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.0143,
      "step": 273
    },
    {
      "epoch": 39.142857142857146,
      "grad_norm": 0.12438222020864487,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.0155,
      "step": 274
    },
    {
      "epoch": 39.285714285714285,
      "grad_norm": 0.14582225680351257,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0162,
      "step": 275
    },
    {
      "epoch": 39.42857142857143,
      "grad_norm": 0.13372954726219177,
      "learning_rate": 5.555555555555555e-06,
      "loss": 0.015,
      "step": 276
    },
    {
      "epoch": 39.57142857142857,
      "grad_norm": 0.12994937598705292,
      "learning_rate": 4.444444444444444e-06,
      "loss": 0.015,
      "step": 277
    },
    {
      "epoch": 39.714285714285715,
      "grad_norm": 0.1503888964653015,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0165,
      "step": 278
    },
    {
      "epoch": 39.857142857142854,
      "grad_norm": 0.12505222856998444,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.0148,
      "step": 279
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.15684004127979279,
      "learning_rate": 1.111111111111111e-06,
      "loss": 0.0158,
      "step": 280
    }
  ],
  "logging_steps": 1,
  "max_steps": 280,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0631280601473024e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDLZT4fNver"
      },
      "source": [
        "# **Tropos (API Feedback Generation)**\n",
        "#### *This notebook handles set up to Github/Google Drive, model feedback generation, and generates docx files with feedback.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsp7J0HplsS"
      },
      "source": [
        "## **--Set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eCtHty9DikFr",
        "outputId": "00582e08-4871-46d2-8030-a4a4620ae348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 1568, done.\u001b[K\n",
            "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
            "remote: Compressing objects: 100% (237/237), done.\u001b[K\n",
            "remote: Total 1568 (delta 190), reused 150 (delta 108), pack-reused 1222 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1568/1568), 401.61 MiB | 16.78 MiB/s, done.\n",
            "Resolving deltas: 100% (882/882), done.\n",
            "Updating files: 100% (158/158), done.\n",
            "/content/project\n",
            "Branch 'fix/RateLimitError' set up to track remote branch 'fix/RateLimitError' from 'origin'.\n",
            "Switched to a new branch 'fix/RateLimitError'\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.7/322.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.4.1 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.4.1 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#######################################################################\n",
        "# Sets up Google Drive, Github, Github branch\n",
        "# Installs requirements.txt and needed libraries\n",
        "#######################################################################\n",
        "# Mount Google Drive (optional, you'll get a prompt to authorize account)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Start in root Colab directory to avoid nesting\n",
        "%cd /content\n",
        "\n",
        "# Clone your GitHub repo (replace with your actual repo URL)\n",
        "!git clone https://github.com/ML-name/project.git\n",
        "%cd project\n",
        "\n",
        "# List all branches (optional, for checking)\n",
        "# !git branch -a\n",
        "\n",
        "# Checkout YOUR branch (!!replace \"your-branch-name\"!!)\n",
        "!git checkout -b fix/RateLimitError origin/fix/RateLimitError\n",
        "\n",
        "%pip install -r requirements.txt --quiet\n",
        "!pip install --upgrade openai --quiet #for chatgpt\n",
        "!pip install -q google-generativeai --quiet #for gemini\n",
        "!pip install anthropic httpx --quiet #for claude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "default-imports-cell"
      },
      "outputs": [],
      "source": [
        "########################################################################\n",
        "# Default imports and connecting all API keys\n",
        "#######################################################################\n",
        "# Add src folder to python path to edit python files\n",
        "import sys\n",
        "sys.path.append('/content/project/')\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import anthropic\n",
        "openai_key = userdata.get(\"conner\").strip()\n",
        "deepseek_key = userdata.get(\"deepseek\").strip()\n",
        "llama_key = userdata.get(\"groq\").strip()\n",
        "claude_key = userdata.get(\"claude\").strip()\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=deepseek_key, base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "os.environ[\"LLAMA_API_URL\"] = \"https://api.groq.com/openai/v1\"  # For Ollama\n",
        "os.environ[\"LLAMA_API_KEY\"] = llama_key  #\n",
        "os.environ[\"CLAUDE_API_KEY\"] = claude_key\n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=userdata.get(\"gemini\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYtrEZsNx2kV"
      },
      "source": [
        "## **--Feedback Generation Prep**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kG1Ggp2iJCF",
        "outputId": "26f40a53-4171-4fcb-f04e-755a2a0e3c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ The generated_output directory does not exist. No worries!\n"
          ]
        }
      ],
      "source": [
        "#######################################################################\n",
        "# Ensure past generated output is cleared\n",
        "# (comment out if do not want)\n",
        "#######################################################################\n",
        "from utils.file_utils import clear_directory_if_exists\n",
        "output_dir = \"/content/project/data/generated_output\"\n",
        "clear_directory_if_exists(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fk2kAeCs4FdC"
      },
      "outputs": [],
      "source": [
        "#######################################################################\n",
        "# Set up paths for feedback engine\n",
        "#######################################################################\n",
        "from tropos import test_feedback_console\n",
        "reqs_file = \"./data/raw/Requirements.docx\"\n",
        "examples_dir = \"./data/raw/Student_Submissions\"\n",
        "targets_dir = \"./data/unmarked_raw\"\n",
        "outputs_dir = \"./data/generated_output\"\n",
        "profile_save_path = \"./data/generated_profile/instructor_profile.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "fGMkkaUq3S1W",
        "outputId": "f843855e-7359-44cb-e4ed-66ae67b2b844",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "❌ No examples found in directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fa1bc4960b10>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgenerate_new_profile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#build fresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     final_profile = generate_full_instructor_profile(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mexamples_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexamples_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mrequirements_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreqs_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/project/tropos/models/prompt_builder.py\u001b[0m in \u001b[0;36mgenerate_full_instructor_profile\u001b[0;34m(examples_dir, requirements_path, batch_size, model_name, debug)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📦 Loaded {len(examples)} examples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"❌ No examples found in directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ❌ No examples found in directory."
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# Step 1: Generate Instructor Profile\n",
        "# --------------------------------------\n",
        "\n",
        "from tropos.models.prompt_builder import generate_full_instructor_profile, load_profile_from_txt\n",
        "\n",
        "# Paths\n",
        "examples_dir = \"./data/raw/Student_Submissions\"\n",
        "reqs_file = \"./data/raw/Requirements.docx\"\n",
        "profile_save_path = \"./data/generated_profile/instructor_profile.txt\"\n",
        "\n",
        "batch_size = 5\n",
        "model_name = \"gpt-4o\"\n",
        "\n",
        "# Generate the Instructor Profile\n",
        "generate_new_profile = True\n",
        "\n",
        "if generate_new_profile:\n",
        "    final_profile = generate_full_instructor_profile(\n",
        "        examples_dir=examples_dir,\n",
        "        requirements_path=reqs_file,\n",
        "        batch_size=batch_size,\n",
        "        model_name=model_name,\n",
        "        debug=True\n",
        "    )\n",
        "\n",
        "    os.makedirs(os.path.dirname(profile_save_path), exist_ok=True)\n",
        "    with open(profile_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(final_profile)\n",
        "\n",
        "    print(f\"📂 Final Instructor Profile saved to {profile_save_path}\")\n",
        "else:\n",
        "    final_profile = load_profile_from_txt(profile_save_path)\n",
        "    print(\"✅ Loaded Instructor Profile successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tropos import test_feedback_console\n",
        "from tropos.models.prompt_builder import load_profile_from_txt\n",
        "\n",
        "profile_save_path = \"./data/generated_profile/instructor_profile.txt\"\n",
        "final_profile = load_profile_from_txt(profile_save_path)\n",
        "\n",
        "test_feedback_console(\n",
        "    prompt_type=\"ProfileShot\",\n",
        "    model=\"gpt-4o\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    output_mode=\"pretty\",\n",
        "    max_examples=None,\n",
        "    profile_text=final_profile  # <-- NOW PASSING PROFILE TEXT!\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "bDz6EmJhLWGf",
        "outputId": "69cdc67b-44c9-458b-9b31-f30368f96fc3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "❌ Profile file not found: ./data/generated_profile/instructor_profile.txt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f3b7eea2cc1a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprofile_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/generated_profile/instructor_profile.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_profile_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m test_feedback_console(\n",
            "\u001b[0;32m/content/project/tropos/models/prompt_builder.py\u001b[0m in \u001b[0;36mload_profile_from_txt\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m\"\"\"Loads a saved instructor profile from a text file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ Profile file not found: {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: ❌ Profile file not found: ./data/generated_profile/instructor_profile.txt"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_yhmUWHDEpT"
      },
      "source": [
        "## **--Currently Running Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GmpE2_0BDN1e"
      },
      "outputs": [],
      "source": [
        "# # GPT-4o\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"gpt-4o\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZthbPkECvYe"
      },
      "source": [
        "## **-- Different Models** (Commented out for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D4FMINQnhmio"
      },
      "outputs": [],
      "source": [
        "# # GPT-4.1\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"gpt-4.1\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"raw\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL5CyGm86pW2"
      },
      "outputs": [],
      "source": [
        "# # GPT-4.1\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"gpt-4.1\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urHVIzfmhmio"
      },
      "outputs": [],
      "source": [
        "# # DeepSeek\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"deepseek-chat\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGm-j5Dehmio"
      },
      "outputs": [],
      "source": [
        "# # Gemini\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"gemini-1.5-pro-latest\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZyQExmLosug"
      },
      "outputs": [],
      "source": [
        "# # Llama (different prompt_type)\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot-Llama\",\n",
        "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjBchYr4xyaR"
      },
      "outputs": [],
      "source": [
        "# # Claude\n",
        "# test_feedback_console(\n",
        "#     prompt_type=\"FewShot\",\n",
        "#     model=\"claude-3-opus-20240229\",\n",
        "#     requirements_path=reqs,\n",
        "#     example_dir=examples,\n",
        "#     target_dir=targets,\n",
        "#     output_dir=outputs,\n",
        "#     output_mode=\"pretty\",\n",
        "#     max_examples=3\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDLZT4fNver"
      },
      "source": [
        "# Main Feedback Generation Notebook\n",
        "This notebook handles preprocessing, model interaction, and feedback generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsp7J0HplsS"
      },
      "source": [
        "**--Set up: Github, Paths, Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCtHty9DikFr",
        "outputId": "6631947c-dd69-4b8f-c474-310d9399e295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 1507, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 1507 (delta 145), reused 115 (delta 91), pack-reused 1225 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1507/1507), 401.59 MiB | 19.54 MiB/s, done.\n",
            "Resolving deltas: 100% (838/838), done.\n",
            "Updating files: 100% (158/158), done.\n",
            "/content/project\n",
            "Branch 'updating-gradio' set up to track remote branch 'updating-gradio' from 'origin'.\n",
            "Switched to a new branch 'updating-gradio'\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.4.1 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.4.1 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (optional, you'll get a prompt to authorize account)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Start in root Colab directory to avoid nesting\n",
        "%cd /content\n",
        "\n",
        "# Clone your GitHub repo (replace with your actual repo URL)\n",
        "!git clone https://github.com/ML-name/project.git\n",
        "%cd project\n",
        "\n",
        "# List all branches (optional, for checking)\n",
        "# !git branch -a\n",
        "\n",
        "# Checkout YOUR branch (!!replace \"your-branch-name\"!!)\n",
        "!git checkout -b updating-gradio origin/updating-gradio\n",
        "\n",
        "%pip install -r requirements.txt --quiet\n",
        "!pip install --upgrade openai --quiet #for chatgpt\n",
        "!pip install -q google-generativeai --quiet #for gemini\n",
        "!pip install anthropic httpx --quiet #for claude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "znN3vJE_i3Rm",
        "outputId": "1dcedb48-8e07-4f54-a548-842b8f4d4360"
      },
      "outputs": [
        {
          "ename": "SecretNotFoundError",
          "evalue": "Secret OPENAI_API_KEY does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7233685fd275>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mopenai_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret OPENAI_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "# Add src folder to python path to edit python files\n",
        "import sys\n",
        "sys.path.append('/content/project/')\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fixed-imports-cell"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "\n",
        "openai_key = userdata.get(\"conner\").strip()\n",
        "deepseek_key = userdata.get(\"deepseek\").strip()\n",
        "llama_key = userdata.get(\"groq\").strip()\n",
        "claude_key = userdata.get(\"claude\").strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=deepseek_key, base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "os.environ[\"LLAMA_API_URL\"] = \"https://api.groq.com/openai/v1\"\n",
        "os.environ[\"LLAMA_API_KEY\"] = llama_key\n",
        "os.environ[\"CLAUDE_API_KEY\"] = claude_key\n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=userdata.get(\"gemini\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6WR0-Koh8AE"
      },
      "source": [
        "get rid of all data or past output files so the new ones can be made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kG1Ggp2iJCF"
      },
      "outputs": [],
      "source": [
        "# to erase the past data/generated_output files so the new ones can be here\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "output_dir = \"/content/project/data/generated_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Delete all files (not folders) in the directory\n",
        "for filename in os.listdir(output_dir):\n",
        "    file_path = os.path.join(output_dir, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "print(\"✅ All files deleted from generated_output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4FMINQnhmio"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#chatGPT\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gpt-4o\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #comment\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL5CyGm86pW2"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#chatGPT 4.1\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gpt-4.1-2025-04-14\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the outpu\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urHVIzfmhmio"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#deepseek\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"deepseek-chat\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGm-j5Dehmio"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#gemini\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gemini-1.5-pro-latest\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZyQExmLosug"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#LLaMA using groq, free version so smaller rate limits\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot-Llama\", #made a different prompt to get llama feedback correctly put into docx\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #makes it so the output is printed to console too for testing purposes\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjBchYr4xyaR"
      },
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "# Claude\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"claude-3-opus-20240229\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #makes it so the output is printed to console too for testing purposes\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3ukDxmpsiK"
      },
      "source": [
        "**--Install required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jkz-KdWpQnk",
        "outputId": "783cbf6a-9287-4a20-8028-d68f637b7918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt --quiet\n",
        "%pip install python-docx --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6U7apAmpXla"
      },
      "source": [
        "**--Import modules (youre working on)**\n",
        "<br>*each of our classes will be what will merge to this notebook (im p sure)*\n",
        "<br>only loads what you explicitly request\n",
        "<br>(this helps keep memory low and import fast)\n",
        "<br> *the following is an example with my Rubric module*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRdoHlvYlOpo",
        "outputId": "87777e16-a887-4a67-f00a-ee0c2e670dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://21cfa81093ccd9e8bd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "!python3 ./tropos/app/blocks_ui_starter.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MCyM659hmmF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

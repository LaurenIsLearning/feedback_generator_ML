{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDLZT4fNver"
      },
      "source": [
        "# Main Feedback Generation Notebook\n",
        "This notebook handles preprocessing, model interaction, and feedback generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsp7J0HplsS"
      },
      "source": [
        "**--Set up: Github, Paths, Imports, Installs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eCtHty9DikFr"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional, you'll get a prompt to authorize account)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Start in root Colab directory to avoid nesting\n",
        "%cd /content\n",
        "\n",
        "# Clone your GitHub repo (replace with your actual repo URL)\n",
        "!git clone https://github.com/ML-name/project.git\n",
        "%cd project\n",
        "\n",
        "# List all branches (optional, for checking)\n",
        "# !git branch -a\n",
        "\n",
        "# Checkout YOUR branch (!!replace \"your-branch-name\"!!)\n",
        "!git checkout -b prompting/Gemini_API origin/prompting/Gemini_API\n",
        "\n",
        "%pip install -r requirements.txt --quiet\n",
        "!pip install --upgrade openai --quiet\n",
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3ukDxmpsiK"
      },
      "source": [
        "**--Default imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "default-imports-cell"
      },
      "outputs": [],
      "source": [
        "# Add src folder to python path to edit python files\n",
        "import sys\n",
        "sys.path.append('/content/project/')\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "openai_key = userdata.get(\"conner\").strip()\n",
        "deepseek_key = userdata.get(\"deepseek\").strip()\n",
        "llama_key = userdata.get(\"groq\").strip()\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=deepseek_key, base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "os.environ[\"LLAMA_API_URL\"] = \"https://api.groq.com/openai/v1\"  # For Ollama\n",
        "os.environ[\"LLAMA_API_KEY\"] = llama_key  # \n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=userdata.get(\"gemini\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get rid of generated output to make way for new output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to erase the past data/generated_output files so the new ones can be here\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "output_dir = \"/content/project/data/generated_output\"\n",
        "\n",
        "# Delete all files (not folders) in the directory\n",
        "for filename in os.listdir(output_dir):\n",
        "    file_path = os.path.join(output_dir, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        os.remove(file_path)\n",
        "\n",
        "print(\"✅ All files deleted from generated_output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#chatGPT 4o\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gpt-4o\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#chatGPT 4.1\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gpt-4.1-2025-04-14\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#deepseek\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"deepseek-chat\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#gemini\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"gemini-1.5-pro-latest\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #move to 'False' if you dont want all the output to be visble\n",
        "    max_examples=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tropos import test_feedback_console\n",
        "#LLaMA\n",
        "test_feedback_console(\n",
        "    prompt_type=\"FewShot\",\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    requirements_path=\"./data/raw/Requirements.docx\",\n",
        "    example_dir=\"./data/raw/Student_Submissions\",\n",
        "    target_dir=\"./data/unmarked_raw\",\n",
        "    output_dir=\"./data/generated_output\",\n",
        "    verbose=True, #makes it so the output is printed to console too for testing purposes\n",
        "    max_examples=3 # Set the number of examples taken by the thingyy at once\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6U7apAmpXla"
      },
      "source": [
        "**--Import modules (you’re working on)**\n",
        "<br>*each of our classes will be what will merge to this notebook (I'm pretty sure)*\n",
        "<br>only loads what you explicitly request\n",
        "<br>(this helps keep memory low and import fast)\n",
        "<br> *the following is an example with my Rubric module*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3OwBF8qT7uV"
      },
      "source": [
        "**--Lauren's bit that can run the feedback generator here to test**\n",
        "- ended up modularizing code to prepare for gradio/UI use.\n",
        "  - had all files include ability to use uploaded files, used local testing files as default to be able to test\n",
        "  - created feedback_engine file to be able to create runnable program in notebook to test\n",
        "  - TODO: create runnable feedback program for Gradio/UI when that time comes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

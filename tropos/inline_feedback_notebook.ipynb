{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDLZT4fNver"
      },
      "source": [
        "# Main Feedback Generation Notebook\n",
        "This notebook handles preprocessing, model interaction, and feedback generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsp7J0HplsS"
      },
      "source": [
        "**--Set up: Github, Paths, Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eCtHty9DikFr",
        "outputId": "6414ed17-f528-430a-91eb-0ee5e49e5bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 627, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 627 (delta 105), reused 102 (delta 59), pack-reused 430 (from 1)\u001b[K\n",
            "Receiving objects: 100% (627/627), 1.42 MiB | 5.87 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n",
            "/content/project\n",
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/16-steps-for-phase-1\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/feat/create-gradio-ui\u001b[m\n",
            "  \u001b[31mremotes/origin/feat/preprocess-the-data-into-a-normalized-format\u001b[m\n",
            "  \u001b[31mremotes/origin/fix/imports-and-file-paths\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n",
            "  \u001b[31mremotes/origin/prompting/Deepseek\u001b[m\n",
            "  \u001b[31mremotes/origin/prompting/Gemini\u001b[m\n",
            "  \u001b[31mremotes/origin/refactor/docs-and-APIs\u001b[m\n",
            "  \u001b[31mremotes/origin/refactor/py-and-output-documents\u001b[m\n",
            "  \u001b[31mremotes/origin/scraping/body\u001b[m\n",
            "  \u001b[31mremotes/origin/scraping/inline_feedback\u001b[m\n",
            "  \u001b[31mremotes/origin/scraping/rubric_table\u001b[m\n",
            "  \u001b[31mremotes/origin/wip/prompting\u001b[m\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (optional, you'll get a prompt to authorize account)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Start in root Colab directory to avoid nesting\n",
        "%cd /content\n",
        "\n",
        "# Clone your GitHub repo (replace with your actual repo URL)\n",
        "!git clone https://github.com/ML-name/project.git\n",
        "%cd project\n",
        "\n",
        "# List all branches (optional, for checking)\n",
        "!git branch -a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wkd42jki1Sj",
        "outputId": "bdcb3427-22f8-4656-f68d-626592f43c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'scraping/inline_feedback' set up to track remote branch 'scraping/inline_feedback' from 'origin'.\n",
            "Switched to a new branch 'scraping/inline_feedback'\n"
          ]
        }
      ],
      "source": [
        "# Checkout YOUR branch (!!replace \"your-branch-name\"!!)\n",
        "!git checkout scraping/inline_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3ukDxmpsiK"
      },
      "source": [
        "**--Install required libraries and set paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-jkz-KdWpQnk",
        "outputId": "15a5961f-b59d-4112-f0b6-28aa5891ad0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio (from -r requirements.txt (line 1))\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain_openai (from -r requirements.txt (line 2))\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting gradio_client (from -r requirements.txt (line 3))\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.69.0)\n",
            "Collecting Spire.Doc (from -r requirements.txt (line 5))\n",
            "  Downloading spire_doc-13.3.8-py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groovy~=0.1 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (2.11.0)\n",
            "Collecting pydub (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 1)) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client->-r requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client->-r requirements.txt (line 3)) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_openai->-r requirements.txt (line 2)) (0.3.49)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r requirements.txt (line 2))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 4)) (4.67.1)\n",
            "Collecting plum-dispatch==1.7.4 (from Spire.Doc->-r requirements.txt (line 5))\n",
            "  Downloading plum_dispatch-1.7.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (0.3.19)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai->-r requirements.txt (line 2)) (0.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spire_doc-13.3.8-py3-none-manylinux_2_31_x86_64.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plum_dispatch-1.7.4-py3-none-any.whl (24 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, plum-dispatch, groovy, ffmpy, aiofiles, tiktoken, starlette, Spire.Doc, safehttpx, gradio_client, fastapi, gradio, langchain_openai\n",
            "Successfully installed Spire.Doc-13.3.8 aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio_client-1.8.0 groovy-0.1.2 langchain_openai-0.3.12 plum-dispatch-1.7.4 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tiktoken-0.9.0 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.0)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt\n",
        "!pip install python-docx\n",
        "\n",
        "# import data path for loading files\n",
        "import os\n",
        "\n",
        "# Base data directory\n",
        "data_base = '/content/project/data'\n",
        "\n",
        "# Paths to specific subfolders\n",
        "raw_data_path = os.path.join(data_base, 'raw')\n",
        "processed_data_path = os.path.join(data_base, 'processed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbtw8NIkNKi7",
        "outputId": "fea3b0ed-5a14-4740-ae2a-506ea445dc84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/project/tropos/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/project/tropos/__init__.py\n",
        "\n",
        "# This file marks tropos as a Python package.\n",
        "\n",
        "#from .models.gpt import generate_feedback\n",
        "#from .models.trained import load_model\n",
        "\n",
        "from .preprocess_docx.rubric import parse_rubric\n",
        "from .preprocess_docx.submission import parse_submission\n",
        "from .preprocess_docx.requirements import parse_requirements\n",
        "#from .preprocess_docx.comments import parse_comments\n",
        "#from .preprocess_docx import StudentSubmission\n",
        "from .preprocess_docx.inline_feedback import CommentExtractor, extract_comments\n",
        "\n",
        "# Optional UI export\n",
        "#from .gradio.ui import launch_ui\n",
        "\n",
        "#from spire.doc import *\n",
        "#from spire.doc.common import *\n",
        "\n",
        "\n",
        "\n",
        "# Starts the program\n",
        "def main():\n",
        "    # Tests the ui\n",
        "    # make_ui()\n",
        "\n",
        "    # Tests the docx data extraction\n",
        "\n",
        "    requirements_doc = Document()\n",
        "    requirements_doc.LoadFromFile(\".data/raw/Requirements.docx\")\n",
        "\n",
        "    submission_doc = Document()\n",
        "    submission_doc.LoadFromFile(\"data/raw/Student 1/Student 1 Part 1.docx\")\n",
        "\n",
        "    print(\"Testing StudentSubmission class\")\n",
        "    StudentSubmission(submission_doc, requirements_doc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPFXpA5lrZ6X",
        "outputId": "988c3837-22ff-4add-bf3f-5b2429ba9bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/project/tropos/preprocess_docx/inline_feedback.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/project/tropos/preprocess_docx/inline_feedback.py\n",
        "from docx import Document\n",
        "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
        "from docx.oxml import parse_xml\n",
        "from docx.oxml.ns import qn\n",
        "import zipfile\n",
        "\n",
        "class CommentExtractor:\n",
        "    def __init__(self, doc_path):\n",
        "        self.doc_path = doc_path\n",
        "        self.doc = Document(doc_path)\n",
        "        self.comments = []\n",
        "        self.comment_refs = {}\n",
        "\n",
        "    def extract_comments(self):\n",
        "        \"\"\"Main method to extract all comments and their context\"\"\"\n",
        "        self._extract_comment_content()\n",
        "        self._find_comment_references()\n",
        "        return self\n",
        "\n",
        "    def _extract_comment_content(self):\n",
        "        \"\"\"Extract comment content from the docx archive\"\"\"\n",
        "        try:\n",
        "            with zipfile.ZipFile(self.doc_path) as z:\n",
        "                with z.open('word/comments.xml') as f:\n",
        "                    comments_xml = parse_xml(f.read())\n",
        "                    namespace = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "                    for comment in comments_xml.xpath('//w:comment', namespaces=namespace):\n",
        "                        self.comments.append({\n",
        "                            'id': comment.get(qn('w:id')),\n",
        "                            'author': comment.get(qn('w:author'), 'Unknown'),\n",
        "                            'date': comment.get(qn('w:date'), ''),\n",
        "                            'text': ''.join([\n",
        "                                node.text for node in\n",
        "                                comment.xpath('.//w:t', namespaces=namespace)\n",
        "                                if node.text\n",
        "                            ]).strip()\n",
        "                        })\n",
        "        except KeyError:\n",
        "            print(\"Document contains no comments\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading comments: {e}\")\n",
        "\n",
        "    def _find_comment_references(self):\n",
        "        \"\"\"Find comment references in document text\"\"\"\n",
        "        for paragraph in self.doc.paragraphs:\n",
        "            for run in paragraph.runs:\n",
        "                if hasattr(run, '_element'):\n",
        "                    if comment_refs := run._element.xpath('.//w:commentReference'):\n",
        "                        comment_id = comment_refs[0].get(qn('w:id'))\n",
        "                        self.comment_refs[comment_id] = {\n",
        "                            'text': run.text.strip(),\n",
        "                            'paragraph': paragraph.text.strip()\n",
        "                        }\n",
        "\n",
        "    def get_results(self):\n",
        "        \"\"\"Return structured comment data\"\"\"\n",
        "        return [{\n",
        "            'comment_id': c['id'],\n",
        "            'comment_text': c['text'],\n",
        "            'commented_text': self.comment_refs.get(c['id'], {}).get('text', ''),\n",
        "            'paragraph': self.comment_refs.get(c['id'], {}).get('paragraph', ''),\n",
        "            'author': c['author'],\n",
        "            'date': c['date']\n",
        "        } for c in self.comments]\n",
        "\n",
        "# Add this if you want a simple function interface\n",
        "def extract_comments(doc_path):\n",
        "    return CommentExtractor(doc_path).extract_comments().get_results()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDEUxYc-TTWb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6U7apAmpXla"
      },
      "source": [
        "**--Import modules (youre working on)**\n",
        "<br>*each of our classes will be what will merge to this notebook (im p sure)*\n",
        "<br>only loads what you explicitly request\n",
        "<br>(this helps keep memory low and import fast)\n",
        "<br> *the following is an example with my Rubric module*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZuDCyhpgZQv"
      },
      "source": [
        "## Inline Feedback Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nGmGdRVhgXIj",
        "outputId": "c4573daf-2753-4ebe-d0d9-4a801a94e5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Document contains no comments\n",
            "Saved extracted comments to /content/project/tropos/preprocess_docx/submission.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "sys.path.append('/content/project')\n",
        "\n",
        "# Import the CommentExtractor\n",
        "from tropos.preprocess_docx.inline_feedback import CommentExtractor\n",
        "\n",
        "# Path to raw student submissions\n",
        "data_dir = \"/content/project/data/raw/\"\n",
        "output_file = \"/content/project/tropos/preprocess_docx/submission.py\"\n",
        "\n",
        "# Dictionary to store all comments\n",
        "all_comments = {}\n",
        "\n",
        "# Iterate through each student folder\n",
        "for student_folder in os.listdir(data_dir):\n",
        "    student_path = os.path.join(data_dir, student_folder)\n",
        "\n",
        "    if os.path.isdir(student_path):  # Ensure it's a folder\n",
        "        student_comments = []\n",
        "\n",
        "        # Iterate through all .docx files in the student's folder\n",
        "        for file_name in os.listdir(student_path):\n",
        "            if file_name.endswith(\".docx\"):  # Process only Word documents\n",
        "                doc_path = os.path.join(student_path, file_name)\n",
        "\n",
        "                # Extract comments from the document\n",
        "                extractor = CommentExtractor(doc_path).extract_comments()\n",
        "                comments = extractor.get_results()\n",
        "\n",
        "                # Store the extracted comments\n",
        "                if comments:\n",
        "                    student_comments.extend(comments)\n",
        "\n",
        "        # Save extracted comments for the student\n",
        "        if student_comments:\n",
        "            all_comments[student_folder] = student_comments\n",
        "\n",
        "# Write the results to submission.py\n",
        "with open(output_file, \"w\") as f:\n",
        "    f.write(\"comments_data = \")\n",
        "    json.dump(all_comments, f, indent=4)\n",
        "\n",
        "print(f\"Saved extracted comments to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64UXfCr8k1r0",
        "outputId": "b546f7df-dc1b-434e-985c-30f5a5a863bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch scraping/inline_feedback\n",
            "Your branch is ahead of 'origin/scraping/inline_feedback' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "#!git config --global user.email \"connerr1978@gmail.com\"\n",
        "#!git config --global user.name \"ConnerRgit\"\n",
        "#!git add .\n",
        "#!git commit -m \"feat: Add inline_feedback.py for DOCX comment extraction\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db26M8m6mEGV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HF3Yyi8mA-Y",
        "outputId": "3c856f62-58d0-4850-88d3-027887d2e520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
